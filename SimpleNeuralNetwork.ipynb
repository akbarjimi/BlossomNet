{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tWngIP5CvyU7H90KXCeYbnI3H7fM6e3c",
      "authorship_tag": "ABX9TyN0xAdd4XLTrMdUU4oG4N8F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akbarjimi/BlossomNet/blob/main/SimpleNeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load Dataset ðŸ”„"
      ],
      "metadata": {
        "id": "lF8gLedNdQv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import csv\n",
        "from typing import List, Tuple\n",
        "import math\n",
        "import pickle"
      ],
      "metadata": {
        "id": "y5hCJB4_FeVh"
      },
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 339,
      "metadata": {
        "id": "d-y8bz9OCL-A"
      },
      "outputs": [],
      "source": [
        "def load_dataset(file_path: str) -> Tuple[List[List[float]], List[float]]:\n",
        "    \"\"\"\n",
        "    Loads the dataset from a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    file_path (str): Path to the dataset file.\n",
        "\n",
        "    Returns:\n",
        "    data (list): Loaded dataset in list format (excluding labels).\n",
        "    labels (list): Corresponding labels.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    labels = []\n",
        "    with open(file_path , 'r') as file:\n",
        "        reader = csv.reader(file)\n",
        "        next(reader)\n",
        "        for row in reader:\n",
        "            data.append([float(feature) for feature in row[:-1]])\n",
        "            labels.append([float(row[0]), row[-1]])\n",
        "    return data, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_dataset(dataset: List) -> bool:\n",
        "    \"\"\"\n",
        "    Validates the structure and integrity of the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    dataset (list): The loaded dataset.\n",
        "\n",
        "    Returns:\n",
        "    bool: True if dataset is valid, False otherwise.\n",
        "    \"\"\"\n",
        "    if len(dataset) == 0:\n",
        "        return False\n",
        "\n",
        "    num_features = len(dataset[0])\n",
        "    for row in dataset:\n",
        "        if len(row) != num_features:\n",
        "            return False\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "br0ElRjWdxiw"
      },
      "execution_count": 340,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_data_loading():\n",
        "    \"\"\"\n",
        "    Tests if the dataset loading function works as expected.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        file_path = '/content/drive/MyDrive/Colab Notebooks/First Neural Network/Iris.csv'\n",
        "        dataset, labels = load_dataset(file_path)\n",
        "        assert validate_dataset(dataset), \"Dataset validation failed.\"\n",
        "        print(\"Data loaded and validated successfully.\")\n",
        "    except AssertionError as error:\n",
        "        print(f\"Test failed: {error}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"File not found. Make sure the dataset file exists at the specified path.\")\n",
        "    except Exception as error:\n",
        "        print(f\"An error occurred: {error}\")"
      ],
      "metadata": {
        "id": "mKGpyOkmd8ke"
      },
      "execution_count": 341,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Normalize the Data ðŸ”„"
      ],
      "metadata": {
        "id": "PBavN6UwmFU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transpose(data: List[List]) -> List[List]:\n",
        "    \"\"\"Transposes a list of lists.\n",
        "\n",
        "    Args:\n",
        "    data: A list of lists.\n",
        "\n",
        "    Returns:\n",
        "    A transposed list of lists.\n",
        "    \"\"\"\n",
        "\n",
        "    return list(zip(*data))"
      ],
      "metadata": {
        "id": "ILbm5BhKp2fJ"
      },
      "execution_count": 342,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def min_max_normalizer(data: List[List[float]]) -> List[List[float]]:\n",
        "    \"\"\"\n",
        "    Applies Min-Max scaling to the dataset, scaling features between 0 and 1.\n",
        "\n",
        "    Parameters:\n",
        "    data (list of lists): The dataset to be normalized.\n",
        "\n",
        "    Returns:\n",
        "    scaled_data (list of lists): Min-max normalized dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    transposed_data = transpose(data)\n",
        "\n",
        "    min_vals = [min(col) for col in transposed_data]\n",
        "    max_vals = [max(col) for col in transposed_data]\n",
        "\n",
        "    scaled_data = []\n",
        "\n",
        "    for row in data:\n",
        "        try:\n",
        "            scaled_row = [(val - min_val) / (max_val - min_val) if (max_val - min_val) != 0 else 0.0 for val, min_val,max_val in zip(row, min_vals, max_vals)]\n",
        "            scaled_data.append(scaled_row)\n",
        "        except ZeroDivisionError:\n",
        "            row_index = data.index(row)\n",
        "            print(f\"ZeroDivisionError encountered in row {row_index}. Values: {row}, {min_vals}, {max_vals}\")\n",
        "            raise ZeroDivisionError\n",
        "\n",
        "    return scaled_data"
      ],
      "metadata": {
        "id": "GHEPq3Qnm84C"
      },
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(dataset: List[List[float]], training_size: float = 0.7, validation_size: float = 0.15) -> Tuple[List[List[float]], List[List[float]], List[List[float]]]:\n",
        "    \"\"\"\n",
        "    Splits the dataset into training, validation, and test sets.\n",
        "\n",
        "    Parameters:\n",
        "    dataset (list of lists): The dataset to be split.\n",
        "    training_size (float): Proportion of data to be used for training.\n",
        "    validation_size (float): Proportion of data to be used for validation.\n",
        "\n",
        "    Returns:\n",
        "    Tuple containing the training, validation, and test sets.\n",
        "    \"\"\"\n",
        "    random.shuffle(dataset)\n",
        "\n",
        "    total_size = len(dataset)\n",
        "    train_end = int(training_size * total_size)\n",
        "    val_end = int((training_size + validation_size) * total_size)\n",
        "\n",
        "    training_data = dataset[:train_end]\n",
        "    validation_data = dataset[train_end:val_end]\n",
        "    test_data = dataset[val_end:]\n",
        "\n",
        "    return training_data, validation_data, test_data"
      ],
      "metadata": {
        "id": "JUk8uAB0k1F1"
      },
      "execution_count": 344,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_split_data():\n",
        "    \"\"\"\n",
        "    Tests if the data splitting function works as expected.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        file_path = '/content/drive/MyDrive/Colab Notebooks/First Neural Network/Iris.csv'\n",
        "        dataset, labels = load_dataset(file_path)\n",
        "        dataset = min_max_normalizer(dataset)\n",
        "        training_data, validation_data, test_data = split_data(dataset)\n",
        "\n",
        "        assert len(training_data) > 0, \"Training data is empty.\"\n",
        "        assert len(validation_data) > 0, \"Validation data is empty.\"\n",
        "        assert len(test_data) > 0, \"Test data is empty.\"\n",
        "        total_size = len(training_data) + len(validation_data) + len(test_data)\n",
        "        assert total_size == len(dataset), \"Data splitting error: sizes do not match.\"\n",
        "\n",
        "        print(\"Data splitting test passed successfully.\")\n",
        "    except AssertionError as error:\n",
        "        print(f\"Test failed: {error}\")\n",
        "    except Exception as error:\n",
        "        print(f\"An error occurred: {error}\")"
      ],
      "metadata": {
        "id": "EH-SJwTEk4PT"
      },
      "execution_count": 345,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Define the Architecture ðŸ—"
      ],
      "metadata": {
        "id": "VUV-OoGf9CoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron:\n",
        "    \"\"\"A simple neuron class with type hints.\"\"\"\n",
        "\n",
        "    def __init__(self, weights: List[float], bias: float = None):\n",
        "        \"\"\"Initializes a neuron with given weights and bias.\n",
        "\n",
        "        Args:\n",
        "            weights: A list of weights for the neuron's inputs.\n",
        "            bias: The bias term for the neuron.\n",
        "        \"\"\"\n",
        "        self.weights = weights\n",
        "        self.bias = bias if bias is not None else random.uniform(-0.1, 0.1)\n",
        "        self.inputs = []\n",
        "        self.output = 0\n",
        "\n",
        "    def forward(self, inputs: List[float]) -> float:\n",
        "        \"\"\"Calculates the output of the neuron.\n",
        "\n",
        "        Args:\n",
        "            inputs: A list of inputs to the neuron.\n",
        "\n",
        "        Returns:\n",
        "            The output of the neuron.\n",
        "        \"\"\"\n",
        "        self.inputs = inputs\n",
        "        weighted_sum = sum([input_ * weight for input_, weight in zip(inputs, self.weights)])\n",
        "        self.output = weighted_sum + self.bias\n",
        "        return self.output\n",
        "\n",
        "    def activation(self, output: float) -> float:\n",
        "        \"\"\"Applies an activation function to the neuron's output.\n",
        "\n",
        "        Args:\n",
        "            output: The output of the neuron.\n",
        "\n",
        "        Returns:\n",
        "            The activated output of the neuron.\n",
        "        \"\"\"\n",
        "        return output\n",
        "\n",
        "    def compute_gradient(self, delta: float) -> List[float]:\n",
        "        \"\"\"Calculates the gradient for the weights using the delta from the next layer.\n",
        "\n",
        "        Args:\n",
        "            delta: The error signal from the next layer.\n",
        "\n",
        "        Returns:\n",
        "            A list of gradients for the weights.\n",
        "        \"\"\"\n",
        "        gradients = [delta * input_ for input_ in self.inputs]\n",
        "        return gradients\n",
        "\n",
        "    def update_weights(self, learning_rate: float, gradients: List[float]):\n",
        "        \"\"\"Updates the weights and bias of the neuron using the computed gradients.\n",
        "\n",
        "        Args:\n",
        "            learning_rate: The learning rate for weight updates.\n",
        "            gradients: The computed gradients for each weight.\n",
        "        \"\"\"\n",
        "        self.weights = [w - learning_rate * g for w, g in zip(self.weights, gradients)]\n",
        "        self.bias -= learning_rate * gradients[-1]\n",
        "\n",
        "    def propagate_error_back(self) -> List[float]:\n",
        "        \"\"\"Propagates the error signal back to the previous layer.\n",
        "\n",
        "        Returns:\n",
        "            A list of error terms to propagate to the previous layer.\n",
        "        \"\"\"\n",
        "        error_signal = self.output * (1 - self.output)\n",
        "        propagated_errors = [error_signal * weight for weight in self.weights]\n",
        "        return propagated_errors"
      ],
      "metadata": {
        "id": "uJCkaPTT90u7"
      },
      "execution_count": 346,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActivationFunctions:\n",
        "    \"\"\"A utility class for activation functions.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x: float) -> float:\n",
        "        \"\"\"Applies the sigmoid activation function.\n",
        "\n",
        "        Args:\n",
        "            x: The input value.\n",
        "\n",
        "        Returns:\n",
        "            The activated value.\n",
        "        \"\"\"\n",
        "        return 1 / (1 + math.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(x:float) -> float:\n",
        "        \"\"\"Applies the rectified linear unit (ReLU) activation function.\n",
        "\n",
        "        Args:\n",
        "            x: The input value.\n",
        "\n",
        "        Returns:\n",
        "            The activated value.\n",
        "        \"\"\"\n",
        "        return max(0,x)\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x: List[float]) -> List[float]:\n",
        "        \"\"\"Applies the softmax activation function.\n",
        "\n",
        "        Args:\n",
        "            x: A list of input values.\n",
        "\n",
        "        Returns:\n",
        "            A list of normalized probabilities.\n",
        "        \"\"\"\n",
        "\n",
        "        exp_values = [math.exp(value) for value in x]\n",
        "        sum_exp_values = sum(exp_values)\n",
        "        return [exp_value / sum_exp_values for exp_value in exp_values]"
      ],
      "metadata": {
        "id": "VLJoUYpDBP2c"
      },
      "execution_count": 347,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "    \"\"\"A layer in a neural network.\"\"\"\n",
        "\n",
        "    def __init__(self, neurons: List[Neuron], activation: ActivationFunctions, is_output_layer: bool = False):\n",
        "        \"\"\"Initializes a layer with the specified number of neurons and inputs.\n",
        "\n",
        "        Args:\n",
        "            neurons: The neurons in the layer.\n",
        "            activation: The activation function to use for the neurons.\n",
        "        \"\"\"\n",
        "        self.neurons = neurons\n",
        "        self.activation = activation\n",
        "        self.is_output_layer = is_output_layer\n",
        "\n",
        "    def forward(self, inputs: List[float]) -> List[float]:\n",
        "        \"\"\"Propagates inputs through the layer.\n",
        "\n",
        "        Args:\n",
        "            inputs: A list of inputs to the layer.\n",
        "\n",
        "        Returns:\n",
        "            A list of outputs from the neurons in the layer.\n",
        "        \"\"\"\n",
        "        if self.is_output_layer:\n",
        "            logits = [neuron.forward(inputs) for neuron in self.neurons]\n",
        "            return self.softmax(logits)\n",
        "        else:\n",
        "            neuron_outputs = [neuron.forward(inputs) for neuron in self.neurons]\n",
        "            activated_outputs = [self.activation(output) for output in neuron_outputs]\n",
        "            return activated_outputs\n",
        "\n",
        "    def backward(self, delta: List[float], learning_rate: float) -> List[float]:\n",
        "        \"\"\"Performs the backward pass and updates weights and biases.\n",
        "\n",
        "        Args:\n",
        "            delta: The error term (gradient) from the next layer.\n",
        "            learning_rate: The learning rate for weight updates.\n",
        "\n",
        "        Returns:\n",
        "            A list of propagated error terms to pass to the previous layer.\n",
        "        \"\"\"\n",
        "        new_delta = []\n",
        "        for i, neuron in enumerate(self.neurons):\n",
        "            neuron_gradient = neuron.compute_gradient(delta[i])\n",
        "\n",
        "            neuron.update_weights(learning_rate)\n",
        "\n",
        "            new_delta.append(neuron.propagate_error_back())\n",
        "\n",
        "        return new_delta"
      ],
      "metadata": {
        "id": "JBJr0gtOE391"
      },
      "execution_count": 348,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    \"\"\"A neural network with multiple layers.\"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, layers: List[Layer], epochs: int, learning_rate: float):\n",
        "        self.layers = layers\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def forward(self, inputs: List[float]) -> List[float]:\n",
        "        \"\"\"Propagates inputs through the entire network.\n",
        "\n",
        "        Args:\n",
        "            inputs: A list of inputs to the network.\n",
        "\n",
        "        Returns:\n",
        "            A list of outputs from the final layer.\n",
        "        \"\"\"\n",
        "        outputs = inputs\n",
        "        for layer in self.layers:\n",
        "            outputs = layer.forward(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def backward(self, targets: List[float], outputs: List[float]):\n",
        "        \"\"\"Performs backpropagation to update the network's weights and biases.\n",
        "\n",
        "        Args:\n",
        "            targets: A list of target outputs.\n",
        "            outputs: A list of outputs from the final forward pass.\n",
        "        \"\"\"\n",
        "        delta = self.loss_derivative(outputs, targets)\n",
        "\n",
        "        for layer in reversed(self.layers):\n",
        "            delta = layer.backward(delta, self.learning_rate)\n",
        "\n",
        "    def compute_loss(self, predicted: List[float], actual: List[float]) -> float:\n",
        "        \"\"\"Calculates the loss for the predictions.\"\"\"\n",
        "        return LossFunction.cross_entropy(predicted, actual)\n",
        "\n",
        "    def loss_derivative(self, outputs: List[float], targets: List[float]) -> List[float]:\n",
        "        \"\"\"Computes the derivative of the loss function.\"\"\"\n",
        "        return [pred - target for pred, target in zip(outputs, targets)]\n",
        "\n",
        "    def train(self, training_data: List[tuple]):\n",
        "        \"\"\"Trains the network on the given data without mini-batches.\n",
        "\n",
        "        Args:\n",
        "            training_data: A list of tuples containing input and target data.\n",
        "        \"\"\"\n",
        "        num_samples = len(training_data)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            total_loss = 0\n",
        "            random.shuffle(training_data)\n",
        "\n",
        "            for inputs, targets in training_data:\n",
        "                outputs = self.forward(inputs)\n",
        "\n",
        "                loss = self.compute_loss(outputs, targets)\n",
        "                total_loss += loss\n",
        "\n",
        "                self.backward(targets, outputs)\n",
        "\n",
        "            avg_loss = total_loss / num_samples\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{self.epochs} complete. Average loss: {avg_loss:.4f}\")\n",
        "\n",
        "    def evaluate(self, test_data: List[tuple]) -> float:\n",
        "        \"\"\"Evaluates the network on the test data.\"\"\"\n",
        "        inputs_batch, targets_batch = zip(*test_data)\n",
        "        predictions = [self.forward(inputs) for inputs in inputs_batch]\n",
        "        accuracy = self.calculate_accuracy(predictions, targets_batch)\n",
        "        return accuracy\n",
        "\n",
        "    def predict(self, new_data: List[float]) -> List[float]:\n",
        "        \"\"\"Predicts the output for new input data.\n",
        "\n",
        "        Args:\n",
        "            new_data: A list of new input data.\n",
        "\n",
        "        Returns:\n",
        "            A list of predicted outputs.\n",
        "        \"\"\"\n",
        "        return self.forward(new_data)\n",
        "\n",
        "    def calculate_accuracy(self, predictions: List[List[float]], targets: List[List[float]]) -> float:\n",
        "        \"\"\"Calculates the accuracy of the model.\"\"\"\n",
        "        correct_predictions = 0\n",
        "        for pred, target in zip(predictions, targets):\n",
        "            predicted_class = np.argmax(pred)\n",
        "            true_class = np.argmax(target)\n",
        "            if predicted_class == true_class:\n",
        "                correct_predictions += 1\n",
        "        return correct_predictions / len(targets)\n",
        "\n",
        "    def save_weights(self, filename: str):\n",
        "        \"\"\"Saves the weights of the network to a file.\"\"\"\n",
        "        weights = [[neuron.weights for neuron in layer.neurons] for layer in self.layers]\n",
        "        biases = [[neuron.bias for neuron in layer.neurons] for layer in self.layers]\n",
        "\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump((weights, biases), f)\n",
        "\n",
        "    def load_weights(self, filename: str):\n",
        "        \"\"\"Loads weights into the network from a file.\"\"\"\n",
        "        with open(filename, 'rb') as f:\n",
        "            weights, biases = pickle.load(f)\n",
        "\n",
        "        for layer, layer_weights, layer_biases in zip(self.layers, weights, biases):\n",
        "            for neuron, neuron_weights, neuron_bias in zip(layer.neurons, layer_weights, layer_biases):\n",
        "                neuron.weights = neuron_weights\n",
        "                neuron.bias = neuron_bias"
      ],
      "metadata": {
        "id": "g3Jq30B-J1Lk"
      },
      "execution_count": 349,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LossFunction:\n",
        "    \"\"\"A utility class for loss functions.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def cross_entropy(predicted_outputs: List[float], actual_outputs: List[float]) -> float:\n",
        "        \"\"\"Calculates the categorical cross-entropy loss.\n",
        "\n",
        "        Args:\n",
        "            predicted_outputs: A list of predicted probabilities for each class.\n",
        "            actual_outputs: A one-hot encoded list of actual output values.\n",
        "\n",
        "        Returns:\n",
        "            The categorical cross-entropy loss.\n",
        "        \"\"\"\n",
        "        # Clip the predicted values to prevent log(0)\n",
        "        predicted_outputs = np.clip(predicted_outputs, 1e-12, 1 - 1e-12)\n",
        "\n",
        "        # Calculate the cross-entropy loss\n",
        "        loss = -sum([actual_output * np.log(predicted_output)\n",
        "                     for predicted_output, actual_output in zip(predicted_outputs, actual_outputs)])\n",
        "        return loss / len(predicted_outputs)\n",
        "\n",
        "    @staticmethod\n",
        "    def mean_squared_error(predicted_outputs: List[float], actual_outputs: List[float]) -> float:\n",
        "        \"\"\"Calculates the mean squared error (MSE).\n",
        "\n",
        "        Args:\n",
        "            predicted_outputs: A list of predicted output values.\n",
        "            actual_outputs: A list of actual output values.\n",
        "\n",
        "        Returns:\n",
        "            The mean squared error.\n",
        "        \"\"\"\n",
        "        squared_errors = [(predicted_output - actual_output) ** 2\n",
        "                          for predicted_output, actual_output in zip(predicted_outputs, actual_outputs)]\n",
        "        return sum(squared_errors) / len(squared_errors)\n"
      ],
      "metadata": {
        "id": "anKsHgS8N-Iv"
      },
      "execution_count": 350,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WeghitsInitializer:\n",
        "    \"\"\"A utility class for weight initialization.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def weights(num_inputs: int) -> List[float]:\n",
        "        \"\"\"Initializes weights with random values uniformly distributed between -0.1 and 0.1.\n",
        "\n",
        "        Args:\n",
        "            num_inputs: The number of inputs to the neuron.\n",
        "\n",
        "        Returns:\n",
        "            A list of initialized weights.\n",
        "        \"\"\"\n",
        "        return [random.uniform(-0.1, 0.1) for _ in range(num_inputs)]\n"
      ],
      "metadata": {
        "id": "da7SbIFhuipG"
      },
      "execution_count": 351,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workflow ðŸ”®: Load Dataset"
      ],
      "metadata": {
        "id": "ycREDLFRmswI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_loading()\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/First Neural Network/Iris.csv'\n",
        "dataset, labels = load_dataset(file_path)\n",
        "test_split_data()\n",
        "training_data, validation_data, test_data = split_data(dataset)\n",
        "print(training_data)\n",
        "print(validation_data)\n",
        "print(test_data)"
      ],
      "metadata": {
        "id": "Rt4OIVV1msNB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79fefc40-e090-4823-936e-4f4274a84609"
      },
      "execution_count": 352,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded and validated successfully.\n",
            "Data splitting test passed successfully.\n",
            "[[41.0, 5.0, 3.5, 1.3, 0.3], [49.0, 5.3, 3.7, 1.5, 0.2], [80.0, 5.7, 2.6, 3.5, 1.0], [143.0, 5.8, 2.7, 5.1, 1.9], [43.0, 4.4, 3.2, 1.3, 0.2], [113.0, 6.8, 3.0, 5.5, 2.1], [5.0, 5.0, 3.6, 1.4, 0.2], [54.0, 5.5, 2.3, 4.0, 1.3], [94.0, 5.0, 2.3, 3.3, 1.0], [44.0, 5.0, 3.5, 1.6, 0.6], [17.0, 5.4, 3.9, 1.3, 0.4], [9.0, 4.4, 2.9, 1.4, 0.2], [145.0, 6.7, 3.3, 5.7, 2.5], [123.0, 7.7, 2.8, 6.7, 2.0], [83.0, 5.8, 2.7, 3.9, 1.2], [13.0, 4.8, 3.0, 1.4, 0.1], [63.0, 6.0, 2.2, 4.0, 1.0], [115.0, 5.8, 2.8, 5.1, 2.4], [26.0, 5.0, 3.0, 1.6, 0.2], [30.0, 4.7, 3.2, 1.6, 0.2], [132.0, 7.9, 3.8, 6.4, 2.0], [97.0, 5.7, 2.9, 4.2, 1.3], [90.0, 5.5, 2.5, 4.0, 1.3], [36.0, 5.0, 3.2, 1.2, 0.2], [101.0, 6.3, 3.3, 6.0, 2.5], [146.0, 6.7, 3.0, 5.2, 2.3], [71.0, 5.9, 3.2, 4.8, 1.8], [57.0, 6.3, 3.3, 4.7, 1.6], [33.0, 5.2, 4.1, 1.5, 0.1], [25.0, 4.8, 3.4, 1.9, 0.2], [130.0, 7.2, 3.0, 5.8, 1.6], [42.0, 4.5, 2.3, 1.3, 0.3], [142.0, 6.9, 3.1, 5.1, 2.3], [82.0, 5.5, 2.4, 3.7, 1.0], [10.0, 4.9, 3.1, 1.5, 0.1], [6.0, 5.4, 3.9, 1.7, 0.4], [3.0, 4.7, 3.2, 1.3, 0.2], [7.0, 4.6, 3.4, 1.4, 0.3], [31.0, 4.8, 3.1, 1.6, 0.2], [144.0, 6.8, 3.2, 5.9, 2.3], [150.0, 5.9, 3.0, 5.1, 1.8], [140.0, 6.9, 3.1, 5.4, 2.1], [86.0, 6.0, 3.4, 4.5, 1.6], [64.0, 6.1, 2.9, 4.7, 1.4], [103.0, 7.1, 3.0, 5.9, 2.1], [11.0, 5.4, 3.7, 1.5, 0.2], [109.0, 6.7, 2.5, 5.8, 1.8], [75.0, 6.4, 2.9, 4.3, 1.3], [40.0, 5.1, 3.4, 1.5, 0.2], [70.0, 5.6, 2.5, 3.9, 1.1], [32.0, 5.4, 3.4, 1.5, 0.4], [77.0, 6.8, 2.8, 4.8, 1.4], [79.0, 6.0, 2.9, 4.5, 1.5], [127.0, 6.2, 2.8, 4.8, 1.8], [102.0, 5.8, 2.7, 5.1, 1.9], [14.0, 4.3, 3.0, 1.1, 0.1], [108.0, 7.3, 2.9, 6.3, 1.8], [137.0, 6.3, 3.4, 5.6, 2.4], [119.0, 7.7, 2.6, 6.9, 2.3], [81.0, 5.5, 2.4, 3.8, 1.1], [39.0, 4.4, 3.0, 1.3, 0.2], [51.0, 7.0, 3.2, 4.7, 1.4], [117.0, 6.5, 3.0, 5.5, 1.8], [76.0, 6.6, 3.0, 4.4, 1.4], [2.0, 4.9, 3.0, 1.4, 0.2], [24.0, 5.1, 3.3, 1.7, 0.5], [45.0, 5.1, 3.8, 1.9, 0.4], [87.0, 6.7, 3.1, 4.7, 1.5], [23.0, 4.6, 3.6, 1.0, 0.2], [98.0, 6.2, 2.9, 4.3, 1.3], [4.0, 4.6, 3.1, 1.5, 0.2], [89.0, 5.6, 3.0, 4.1, 1.3], [125.0, 6.7, 3.3, 5.7, 2.1], [95.0, 5.6, 2.7, 4.2, 1.3], [65.0, 5.6, 2.9, 3.6, 1.3], [116.0, 6.4, 3.2, 5.3, 2.3], [118.0, 7.7, 3.8, 6.7, 2.2], [134.0, 6.3, 2.8, 5.1, 1.5], [121.0, 6.9, 3.2, 5.7, 2.3], [91.0, 5.5, 2.6, 4.4, 1.2], [148.0, 6.5, 3.0, 5.2, 2.0], [21.0, 5.4, 3.4, 1.7, 0.2], [114.0, 5.7, 2.5, 5.0, 2.0], [131.0, 7.4, 2.8, 6.1, 1.9], [38.0, 4.9, 3.1, 1.5, 0.1], [107.0, 4.9, 2.5, 4.5, 1.7], [12.0, 4.8, 3.4, 1.6, 0.2], [84.0, 6.0, 2.7, 5.1, 1.6], [128.0, 6.1, 3.0, 4.9, 1.8], [106.0, 7.6, 3.0, 6.6, 2.1], [18.0, 5.1, 3.5, 1.4, 0.3], [110.0, 7.2, 3.6, 6.1, 2.5], [15.0, 5.8, 4.0, 1.2, 0.2], [22.0, 5.1, 3.7, 1.5, 0.4], [48.0, 4.6, 3.2, 1.4, 0.2], [141.0, 6.7, 3.1, 5.6, 2.4], [68.0, 5.8, 2.7, 4.1, 1.0], [35.0, 4.9, 3.1, 1.5, 0.1], [28.0, 5.2, 3.5, 1.5, 0.2], [60.0, 5.2, 2.7, 3.9, 1.4], [8.0, 5.0, 3.4, 1.5, 0.2], [19.0, 5.7, 3.8, 1.7, 0.3], [55.0, 6.5, 2.8, 4.6, 1.5], [99.0, 5.1, 2.5, 3.0, 1.1], [105.0, 6.5, 3.0, 5.8, 2.2]]\n",
            "[[136.0, 7.7, 3.0, 6.1, 2.3], [37.0, 5.5, 3.5, 1.3, 0.2], [129.0, 6.4, 2.8, 5.6, 2.1], [92.0, 6.1, 3.0, 4.6, 1.4], [56.0, 5.7, 2.8, 4.5, 1.3], [78.0, 6.7, 3.0, 5.0, 1.7], [133.0, 6.4, 2.8, 5.6, 2.2], [16.0, 5.7, 4.4, 1.5, 0.4], [88.0, 6.3, 2.3, 4.4, 1.3], [120.0, 6.0, 2.2, 5.0, 1.5], [69.0, 6.2, 2.2, 4.5, 1.5], [58.0, 4.9, 2.4, 3.3, 1.0], [46.0, 4.8, 3.0, 1.4, 0.3], [50.0, 5.0, 3.3, 1.4, 0.2], [104.0, 6.3, 2.9, 5.6, 1.8], [52.0, 6.4, 3.2, 4.5, 1.5], [126.0, 7.2, 3.2, 6.0, 1.8], [59.0, 6.6, 2.9, 4.6, 1.3], [72.0, 6.1, 2.8, 4.0, 1.3], [124.0, 6.3, 2.7, 4.9, 1.8], [61.0, 5.0, 2.0, 3.5, 1.0], [147.0, 6.3, 2.5, 5.0, 1.9]]\n",
            "[[66.0, 6.7, 3.1, 4.4, 1.4], [62.0, 5.9, 3.0, 4.2, 1.5], [96.0, 5.7, 3.0, 4.2, 1.2], [47.0, 5.1, 3.8, 1.6, 0.2], [73.0, 6.3, 2.5, 4.9, 1.5], [93.0, 5.8, 2.6, 4.0, 1.2], [20.0, 5.1, 3.8, 1.5, 0.3], [1.0, 5.1, 3.5, 1.4, 0.2], [111.0, 6.5, 3.2, 5.1, 2.0], [29.0, 5.2, 3.4, 1.4, 0.2], [67.0, 5.6, 3.0, 4.5, 1.5], [149.0, 6.2, 3.4, 5.4, 2.3], [138.0, 6.4, 3.1, 5.5, 1.8], [100.0, 5.7, 2.8, 4.1, 1.3], [85.0, 5.4, 3.0, 4.5, 1.5], [27.0, 5.0, 3.4, 1.6, 0.4], [122.0, 5.6, 2.8, 4.9, 2.0], [139.0, 6.0, 3.0, 4.8, 1.8], [74.0, 6.1, 2.8, 4.7, 1.2], [112.0, 6.4, 2.7, 5.3, 1.9], [135.0, 6.1, 2.6, 5.6, 1.4], [34.0, 5.5, 4.2, 1.4, 0.2], [53.0, 6.9, 3.1, 4.9, 1.5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workflow ðŸ”®: Architecture"
      ],
      "metadata": {
        "id": "Iz1cl6tesGG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create input layer"
      ],
      "metadata": {
        "id": "r5fEpiz3swJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = WeghitsInitializer.weights(4)\n",
        "bias = random.uniform(-0.1, 0.1)\n",
        "input_neurons = [Neuron(weights=weights, bias=bias) for _ in range(4)]\n",
        "input_layer = Layer(neurons=input_neurons, activation= ActivationFunctions.relu, is_output_layer=False)\n",
        "\n",
        "# Print weights of input neurons\n",
        "for neuron in input_layer.neurons:\n",
        "    print(\"Input Neuron Weights:\", neuron.weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrEKj4tMsYN8",
        "outputId": "d6bb579d-3b33-4c81-8ef4-f64cd9be7e04"
      },
      "execution_count": 353,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Neuron Weights: [0.09965488877573989, 0.08715885344261251, -0.010285813533602958, 0.05398284950311161]\n",
            "Input Neuron Weights: [0.09965488877573989, 0.08715885344261251, -0.010285813533602958, 0.05398284950311161]\n",
            "Input Neuron Weights: [0.09965488877573989, 0.08715885344261251, -0.010285813533602958, 0.05398284950311161]\n",
            "Input Neuron Weights: [0.09965488877573989, 0.08715885344261251, -0.010285813533602958, 0.05398284950311161]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create hidden layer"
      ],
      "metadata": {
        "id": "lnY2OyY8wQI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = WeghitsInitializer.weights(4)\n",
        "bias = random.uniform(-0.1, 0.1)\n",
        "hidden_neurons = [Neuron(weights=weights, bias=bias) for _ in range(5)]\n",
        "hidden_layer = Layer(neurons=hidden_neurons, activation= ActivationFunctions.relu, is_output_layer=False)\n",
        "\n",
        "# Print weights of input neurons\n",
        "for neuron in hidden_layer.neurons:\n",
        "    print(\"Input Neuron Weights:\", neuron.weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bK5AUOBwUWP",
        "outputId": "60b22a07-43e3-400f-f2cb-5cfcb7a11732"
      },
      "execution_count": 354,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Neuron Weights: [-0.035639272242701456, 0.08088801537272478, 0.05910702720905972, 0.0018266511431209442]\n",
            "Input Neuron Weights: [-0.035639272242701456, 0.08088801537272478, 0.05910702720905972, 0.0018266511431209442]\n",
            "Input Neuron Weights: [-0.035639272242701456, 0.08088801537272478, 0.05910702720905972, 0.0018266511431209442]\n",
            "Input Neuron Weights: [-0.035639272242701456, 0.08088801537272478, 0.05910702720905972, 0.0018266511431209442]\n",
            "Input Neuron Weights: [-0.035639272242701456, 0.08088801537272478, 0.05910702720905972, 0.0018266511431209442]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create output layer"
      ],
      "metadata": {
        "id": "UmGT2bpKw1in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = WeghitsInitializer.weights(5)\n",
        "bias = random.uniform(-0.1, 0.1)\n",
        "output_neurons = [Neuron(weights=weights, bias=bias) for _ in range(3)]\n",
        "output_layer = Layer(neurons=output_neurons, activation= ActivationFunctions.relu, is_output_layer=True)\n",
        "\n",
        "# Print weights of input neurons\n",
        "for neuron in output_layer.neurons:\n",
        "    print(\"Input Neuron Weights:\", neuron.weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKPIAROvw2kq",
        "outputId": "9d24db97-895d-4b56-fa5f-0fae6c1bfa5f"
      },
      "execution_count": 355,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Neuron Weights: [0.09275175309360151, -0.025779140717981444, 0.01783477137755371, 0.04088030267586745, -0.032747154947185816]\n",
            "Input Neuron Weights: [0.09275175309360151, -0.025779140717981444, 0.01783477137755371, 0.04088030267586745, -0.032747154947185816]\n",
            "Input Neuron Weights: [0.09275175309360151, -0.025779140717981444, 0.01783477137755371, 0.04088030267586745, -0.032747154947185816]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "n-c1fLTl18Ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define network architecture\n",
        "epochs = 100\n",
        "learning_rate = 0.01\n",
        "layers = [input_layer, hidden_layer, output_layer]\n",
        "\n",
        "# Initialize the network\n",
        "network = Network(layers=layers, epochs=epochs, learning_rate=learning_rate)\n",
        "\n",
        "num_samples = len(training_data)\n",
        "print(num_samples)\n",
        "print(training_data)\n",
        "print(labels)\n",
        "\n",
        "def match_and_combine(upper_list: List[List[float]], lower_list: List[List[float]]) -> List[Tuple[List[float], str]]:\n",
        "    # Create a dictionary from the lower list\n",
        "    lower_dict = {item[0]: item[1] for item in lower_list}\n",
        "\n",
        "    # Create a list to store the result tuples\n",
        "    result = []\n",
        "\n",
        "    # Iterate through the upper list\n",
        "    for item in upper_list:\n",
        "        key = item[0]\n",
        "        # Check if the key exists in the lower dictionary\n",
        "        if key in lower_dict:\n",
        "            # Create a tuple (without the key) and add to the result list\n",
        "            result.append((item[1:], lower_dict[key]))\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example usage\n",
        "combined = match_and_combine(training_data, labels)\n",
        "print(combined)\n",
        "\n",
        "network.train(combined)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "9Wl2HNYh19rX",
        "outputId": "e81f8e14-794b-411a-f583-832df6eb4d4c"
      },
      "execution_count": 358,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105\n",
            "[[41.0, 5.0, 3.5, 1.3, 0.3], [49.0, 5.3, 3.7, 1.5, 0.2], [80.0, 5.7, 2.6, 3.5, 1.0], [143.0, 5.8, 2.7, 5.1, 1.9], [43.0, 4.4, 3.2, 1.3, 0.2], [113.0, 6.8, 3.0, 5.5, 2.1], [5.0, 5.0, 3.6, 1.4, 0.2], [54.0, 5.5, 2.3, 4.0, 1.3], [94.0, 5.0, 2.3, 3.3, 1.0], [44.0, 5.0, 3.5, 1.6, 0.6], [17.0, 5.4, 3.9, 1.3, 0.4], [9.0, 4.4, 2.9, 1.4, 0.2], [145.0, 6.7, 3.3, 5.7, 2.5], [123.0, 7.7, 2.8, 6.7, 2.0], [83.0, 5.8, 2.7, 3.9, 1.2], [13.0, 4.8, 3.0, 1.4, 0.1], [63.0, 6.0, 2.2, 4.0, 1.0], [115.0, 5.8, 2.8, 5.1, 2.4], [26.0, 5.0, 3.0, 1.6, 0.2], [30.0, 4.7, 3.2, 1.6, 0.2], [132.0, 7.9, 3.8, 6.4, 2.0], [97.0, 5.7, 2.9, 4.2, 1.3], [90.0, 5.5, 2.5, 4.0, 1.3], [36.0, 5.0, 3.2, 1.2, 0.2], [101.0, 6.3, 3.3, 6.0, 2.5], [146.0, 6.7, 3.0, 5.2, 2.3], [71.0, 5.9, 3.2, 4.8, 1.8], [57.0, 6.3, 3.3, 4.7, 1.6], [33.0, 5.2, 4.1, 1.5, 0.1], [25.0, 4.8, 3.4, 1.9, 0.2], [130.0, 7.2, 3.0, 5.8, 1.6], [42.0, 4.5, 2.3, 1.3, 0.3], [142.0, 6.9, 3.1, 5.1, 2.3], [82.0, 5.5, 2.4, 3.7, 1.0], [10.0, 4.9, 3.1, 1.5, 0.1], [6.0, 5.4, 3.9, 1.7, 0.4], [3.0, 4.7, 3.2, 1.3, 0.2], [7.0, 4.6, 3.4, 1.4, 0.3], [31.0, 4.8, 3.1, 1.6, 0.2], [144.0, 6.8, 3.2, 5.9, 2.3], [150.0, 5.9, 3.0, 5.1, 1.8], [140.0, 6.9, 3.1, 5.4, 2.1], [86.0, 6.0, 3.4, 4.5, 1.6], [64.0, 6.1, 2.9, 4.7, 1.4], [103.0, 7.1, 3.0, 5.9, 2.1], [11.0, 5.4, 3.7, 1.5, 0.2], [109.0, 6.7, 2.5, 5.8, 1.8], [75.0, 6.4, 2.9, 4.3, 1.3], [40.0, 5.1, 3.4, 1.5, 0.2], [70.0, 5.6, 2.5, 3.9, 1.1], [32.0, 5.4, 3.4, 1.5, 0.4], [77.0, 6.8, 2.8, 4.8, 1.4], [79.0, 6.0, 2.9, 4.5, 1.5], [127.0, 6.2, 2.8, 4.8, 1.8], [102.0, 5.8, 2.7, 5.1, 1.9], [14.0, 4.3, 3.0, 1.1, 0.1], [108.0, 7.3, 2.9, 6.3, 1.8], [137.0, 6.3, 3.4, 5.6, 2.4], [119.0, 7.7, 2.6, 6.9, 2.3], [81.0, 5.5, 2.4, 3.8, 1.1], [39.0, 4.4, 3.0, 1.3, 0.2], [51.0, 7.0, 3.2, 4.7, 1.4], [117.0, 6.5, 3.0, 5.5, 1.8], [76.0, 6.6, 3.0, 4.4, 1.4], [2.0, 4.9, 3.0, 1.4, 0.2], [24.0, 5.1, 3.3, 1.7, 0.5], [45.0, 5.1, 3.8, 1.9, 0.4], [87.0, 6.7, 3.1, 4.7, 1.5], [23.0, 4.6, 3.6, 1.0, 0.2], [98.0, 6.2, 2.9, 4.3, 1.3], [4.0, 4.6, 3.1, 1.5, 0.2], [89.0, 5.6, 3.0, 4.1, 1.3], [125.0, 6.7, 3.3, 5.7, 2.1], [95.0, 5.6, 2.7, 4.2, 1.3], [65.0, 5.6, 2.9, 3.6, 1.3], [116.0, 6.4, 3.2, 5.3, 2.3], [118.0, 7.7, 3.8, 6.7, 2.2], [134.0, 6.3, 2.8, 5.1, 1.5], [121.0, 6.9, 3.2, 5.7, 2.3], [91.0, 5.5, 2.6, 4.4, 1.2], [148.0, 6.5, 3.0, 5.2, 2.0], [21.0, 5.4, 3.4, 1.7, 0.2], [114.0, 5.7, 2.5, 5.0, 2.0], [131.0, 7.4, 2.8, 6.1, 1.9], [38.0, 4.9, 3.1, 1.5, 0.1], [107.0, 4.9, 2.5, 4.5, 1.7], [12.0, 4.8, 3.4, 1.6, 0.2], [84.0, 6.0, 2.7, 5.1, 1.6], [128.0, 6.1, 3.0, 4.9, 1.8], [106.0, 7.6, 3.0, 6.6, 2.1], [18.0, 5.1, 3.5, 1.4, 0.3], [110.0, 7.2, 3.6, 6.1, 2.5], [15.0, 5.8, 4.0, 1.2, 0.2], [22.0, 5.1, 3.7, 1.5, 0.4], [48.0, 4.6, 3.2, 1.4, 0.2], [141.0, 6.7, 3.1, 5.6, 2.4], [68.0, 5.8, 2.7, 4.1, 1.0], [35.0, 4.9, 3.1, 1.5, 0.1], [28.0, 5.2, 3.5, 1.5, 0.2], [60.0, 5.2, 2.7, 3.9, 1.4], [8.0, 5.0, 3.4, 1.5, 0.2], [19.0, 5.7, 3.8, 1.7, 0.3], [55.0, 6.5, 2.8, 4.6, 1.5], [99.0, 5.1, 2.5, 3.0, 1.1], [105.0, 6.5, 3.0, 5.8, 2.2]]\n",
            "[[1.0, 'Iris-setosa'], [2.0, 'Iris-setosa'], [3.0, 'Iris-setosa'], [4.0, 'Iris-setosa'], [5.0, 'Iris-setosa'], [6.0, 'Iris-setosa'], [7.0, 'Iris-setosa'], [8.0, 'Iris-setosa'], [9.0, 'Iris-setosa'], [10.0, 'Iris-setosa'], [11.0, 'Iris-setosa'], [12.0, 'Iris-setosa'], [13.0, 'Iris-setosa'], [14.0, 'Iris-setosa'], [15.0, 'Iris-setosa'], [16.0, 'Iris-setosa'], [17.0, 'Iris-setosa'], [18.0, 'Iris-setosa'], [19.0, 'Iris-setosa'], [20.0, 'Iris-setosa'], [21.0, 'Iris-setosa'], [22.0, 'Iris-setosa'], [23.0, 'Iris-setosa'], [24.0, 'Iris-setosa'], [25.0, 'Iris-setosa'], [26.0, 'Iris-setosa'], [27.0, 'Iris-setosa'], [28.0, 'Iris-setosa'], [29.0, 'Iris-setosa'], [30.0, 'Iris-setosa'], [31.0, 'Iris-setosa'], [32.0, 'Iris-setosa'], [33.0, 'Iris-setosa'], [34.0, 'Iris-setosa'], [35.0, 'Iris-setosa'], [36.0, 'Iris-setosa'], [37.0, 'Iris-setosa'], [38.0, 'Iris-setosa'], [39.0, 'Iris-setosa'], [40.0, 'Iris-setosa'], [41.0, 'Iris-setosa'], [42.0, 'Iris-setosa'], [43.0, 'Iris-setosa'], [44.0, 'Iris-setosa'], [45.0, 'Iris-setosa'], [46.0, 'Iris-setosa'], [47.0, 'Iris-setosa'], [48.0, 'Iris-setosa'], [49.0, 'Iris-setosa'], [50.0, 'Iris-setosa'], [51.0, 'Iris-versicolor'], [52.0, 'Iris-versicolor'], [53.0, 'Iris-versicolor'], [54.0, 'Iris-versicolor'], [55.0, 'Iris-versicolor'], [56.0, 'Iris-versicolor'], [57.0, 'Iris-versicolor'], [58.0, 'Iris-versicolor'], [59.0, 'Iris-versicolor'], [60.0, 'Iris-versicolor'], [61.0, 'Iris-versicolor'], [62.0, 'Iris-versicolor'], [63.0, 'Iris-versicolor'], [64.0, 'Iris-versicolor'], [65.0, 'Iris-versicolor'], [66.0, 'Iris-versicolor'], [67.0, 'Iris-versicolor'], [68.0, 'Iris-versicolor'], [69.0, 'Iris-versicolor'], [70.0, 'Iris-versicolor'], [71.0, 'Iris-versicolor'], [72.0, 'Iris-versicolor'], [73.0, 'Iris-versicolor'], [74.0, 'Iris-versicolor'], [75.0, 'Iris-versicolor'], [76.0, 'Iris-versicolor'], [77.0, 'Iris-versicolor'], [78.0, 'Iris-versicolor'], [79.0, 'Iris-versicolor'], [80.0, 'Iris-versicolor'], [81.0, 'Iris-versicolor'], [82.0, 'Iris-versicolor'], [83.0, 'Iris-versicolor'], [84.0, 'Iris-versicolor'], [85.0, 'Iris-versicolor'], [86.0, 'Iris-versicolor'], [87.0, 'Iris-versicolor'], [88.0, 'Iris-versicolor'], [89.0, 'Iris-versicolor'], [90.0, 'Iris-versicolor'], [91.0, 'Iris-versicolor'], [92.0, 'Iris-versicolor'], [93.0, 'Iris-versicolor'], [94.0, 'Iris-versicolor'], [95.0, 'Iris-versicolor'], [96.0, 'Iris-versicolor'], [97.0, 'Iris-versicolor'], [98.0, 'Iris-versicolor'], [99.0, 'Iris-versicolor'], [100.0, 'Iris-versicolor'], [101.0, 'Iris-virginica'], [102.0, 'Iris-virginica'], [103.0, 'Iris-virginica'], [104.0, 'Iris-virginica'], [105.0, 'Iris-virginica'], [106.0, 'Iris-virginica'], [107.0, 'Iris-virginica'], [108.0, 'Iris-virginica'], [109.0, 'Iris-virginica'], [110.0, 'Iris-virginica'], [111.0, 'Iris-virginica'], [112.0, 'Iris-virginica'], [113.0, 'Iris-virginica'], [114.0, 'Iris-virginica'], [115.0, 'Iris-virginica'], [116.0, 'Iris-virginica'], [117.0, 'Iris-virginica'], [118.0, 'Iris-virginica'], [119.0, 'Iris-virginica'], [120.0, 'Iris-virginica'], [121.0, 'Iris-virginica'], [122.0, 'Iris-virginica'], [123.0, 'Iris-virginica'], [124.0, 'Iris-virginica'], [125.0, 'Iris-virginica'], [126.0, 'Iris-virginica'], [127.0, 'Iris-virginica'], [128.0, 'Iris-virginica'], [129.0, 'Iris-virginica'], [130.0, 'Iris-virginica'], [131.0, 'Iris-virginica'], [132.0, 'Iris-virginica'], [133.0, 'Iris-virginica'], [134.0, 'Iris-virginica'], [135.0, 'Iris-virginica'], [136.0, 'Iris-virginica'], [137.0, 'Iris-virginica'], [138.0, 'Iris-virginica'], [139.0, 'Iris-virginica'], [140.0, 'Iris-virginica'], [141.0, 'Iris-virginica'], [142.0, 'Iris-virginica'], [143.0, 'Iris-virginica'], [144.0, 'Iris-virginica'], [145.0, 'Iris-virginica'], [146.0, 'Iris-virginica'], [147.0, 'Iris-virginica'], [148.0, 'Iris-virginica'], [149.0, 'Iris-virginica'], [150.0, 'Iris-virginica']]\n",
            "[([5.0, 3.5, 1.3, 0.3], 'Iris-setosa'), ([5.3, 3.7, 1.5, 0.2], 'Iris-setosa'), ([5.7, 2.6, 3.5, 1.0], 'Iris-versicolor'), ([5.8, 2.7, 5.1, 1.9], 'Iris-virginica'), ([4.4, 3.2, 1.3, 0.2], 'Iris-setosa'), ([6.8, 3.0, 5.5, 2.1], 'Iris-virginica'), ([5.0, 3.6, 1.4, 0.2], 'Iris-setosa'), ([5.5, 2.3, 4.0, 1.3], 'Iris-versicolor'), ([5.0, 2.3, 3.3, 1.0], 'Iris-versicolor'), ([5.0, 3.5, 1.6, 0.6], 'Iris-setosa'), ([5.4, 3.9, 1.3, 0.4], 'Iris-setosa'), ([4.4, 2.9, 1.4, 0.2], 'Iris-setosa'), ([6.7, 3.3, 5.7, 2.5], 'Iris-virginica'), ([7.7, 2.8, 6.7, 2.0], 'Iris-virginica'), ([5.8, 2.7, 3.9, 1.2], 'Iris-versicolor'), ([4.8, 3.0, 1.4, 0.1], 'Iris-setosa'), ([6.0, 2.2, 4.0, 1.0], 'Iris-versicolor'), ([5.8, 2.8, 5.1, 2.4], 'Iris-virginica'), ([5.0, 3.0, 1.6, 0.2], 'Iris-setosa'), ([4.7, 3.2, 1.6, 0.2], 'Iris-setosa'), ([7.9, 3.8, 6.4, 2.0], 'Iris-virginica'), ([5.7, 2.9, 4.2, 1.3], 'Iris-versicolor'), ([5.5, 2.5, 4.0, 1.3], 'Iris-versicolor'), ([5.0, 3.2, 1.2, 0.2], 'Iris-setosa'), ([6.3, 3.3, 6.0, 2.5], 'Iris-virginica'), ([6.7, 3.0, 5.2, 2.3], 'Iris-virginica'), ([5.9, 3.2, 4.8, 1.8], 'Iris-versicolor'), ([6.3, 3.3, 4.7, 1.6], 'Iris-versicolor'), ([5.2, 4.1, 1.5, 0.1], 'Iris-setosa'), ([4.8, 3.4, 1.9, 0.2], 'Iris-setosa'), ([7.2, 3.0, 5.8, 1.6], 'Iris-virginica'), ([4.5, 2.3, 1.3, 0.3], 'Iris-setosa'), ([6.9, 3.1, 5.1, 2.3], 'Iris-virginica'), ([5.5, 2.4, 3.7, 1.0], 'Iris-versicolor'), ([4.9, 3.1, 1.5, 0.1], 'Iris-setosa'), ([5.4, 3.9, 1.7, 0.4], 'Iris-setosa'), ([4.7, 3.2, 1.3, 0.2], 'Iris-setosa'), ([4.6, 3.4, 1.4, 0.3], 'Iris-setosa'), ([4.8, 3.1, 1.6, 0.2], 'Iris-setosa'), ([6.8, 3.2, 5.9, 2.3], 'Iris-virginica'), ([5.9, 3.0, 5.1, 1.8], 'Iris-virginica'), ([6.9, 3.1, 5.4, 2.1], 'Iris-virginica'), ([6.0, 3.4, 4.5, 1.6], 'Iris-versicolor'), ([6.1, 2.9, 4.7, 1.4], 'Iris-versicolor'), ([7.1, 3.0, 5.9, 2.1], 'Iris-virginica'), ([5.4, 3.7, 1.5, 0.2], 'Iris-setosa'), ([6.7, 2.5, 5.8, 1.8], 'Iris-virginica'), ([6.4, 2.9, 4.3, 1.3], 'Iris-versicolor'), ([5.1, 3.4, 1.5, 0.2], 'Iris-setosa'), ([5.6, 2.5, 3.9, 1.1], 'Iris-versicolor'), ([5.4, 3.4, 1.5, 0.4], 'Iris-setosa'), ([6.8, 2.8, 4.8, 1.4], 'Iris-versicolor'), ([6.0, 2.9, 4.5, 1.5], 'Iris-versicolor'), ([6.2, 2.8, 4.8, 1.8], 'Iris-virginica'), ([5.8, 2.7, 5.1, 1.9], 'Iris-virginica'), ([4.3, 3.0, 1.1, 0.1], 'Iris-setosa'), ([7.3, 2.9, 6.3, 1.8], 'Iris-virginica'), ([6.3, 3.4, 5.6, 2.4], 'Iris-virginica'), ([7.7, 2.6, 6.9, 2.3], 'Iris-virginica'), ([5.5, 2.4, 3.8, 1.1], 'Iris-versicolor'), ([4.4, 3.0, 1.3, 0.2], 'Iris-setosa'), ([7.0, 3.2, 4.7, 1.4], 'Iris-versicolor'), ([6.5, 3.0, 5.5, 1.8], 'Iris-virginica'), ([6.6, 3.0, 4.4, 1.4], 'Iris-versicolor'), ([4.9, 3.0, 1.4, 0.2], 'Iris-setosa'), ([5.1, 3.3, 1.7, 0.5], 'Iris-setosa'), ([5.1, 3.8, 1.9, 0.4], 'Iris-setosa'), ([6.7, 3.1, 4.7, 1.5], 'Iris-versicolor'), ([4.6, 3.6, 1.0, 0.2], 'Iris-setosa'), ([6.2, 2.9, 4.3, 1.3], 'Iris-versicolor'), ([4.6, 3.1, 1.5, 0.2], 'Iris-setosa'), ([5.6, 3.0, 4.1, 1.3], 'Iris-versicolor'), ([6.7, 3.3, 5.7, 2.1], 'Iris-virginica'), ([5.6, 2.7, 4.2, 1.3], 'Iris-versicolor'), ([5.6, 2.9, 3.6, 1.3], 'Iris-versicolor'), ([6.4, 3.2, 5.3, 2.3], 'Iris-virginica'), ([7.7, 3.8, 6.7, 2.2], 'Iris-virginica'), ([6.3, 2.8, 5.1, 1.5], 'Iris-virginica'), ([6.9, 3.2, 5.7, 2.3], 'Iris-virginica'), ([5.5, 2.6, 4.4, 1.2], 'Iris-versicolor'), ([6.5, 3.0, 5.2, 2.0], 'Iris-virginica'), ([5.4, 3.4, 1.7, 0.2], 'Iris-setosa'), ([5.7, 2.5, 5.0, 2.0], 'Iris-virginica'), ([7.4, 2.8, 6.1, 1.9], 'Iris-virginica'), ([4.9, 3.1, 1.5, 0.1], 'Iris-setosa'), ([4.9, 2.5, 4.5, 1.7], 'Iris-virginica'), ([4.8, 3.4, 1.6, 0.2], 'Iris-setosa'), ([6.0, 2.7, 5.1, 1.6], 'Iris-versicolor'), ([6.1, 3.0, 4.9, 1.8], 'Iris-virginica'), ([7.6, 3.0, 6.6, 2.1], 'Iris-virginica'), ([5.1, 3.5, 1.4, 0.3], 'Iris-setosa'), ([7.2, 3.6, 6.1, 2.5], 'Iris-virginica'), ([5.8, 4.0, 1.2, 0.2], 'Iris-setosa'), ([5.1, 3.7, 1.5, 0.4], 'Iris-setosa'), ([4.6, 3.2, 1.4, 0.2], 'Iris-setosa'), ([6.7, 3.1, 5.6, 2.4], 'Iris-virginica'), ([5.8, 2.7, 4.1, 1.0], 'Iris-versicolor'), ([4.9, 3.1, 1.5, 0.1], 'Iris-setosa'), ([5.2, 3.5, 1.5, 0.2], 'Iris-setosa'), ([5.2, 2.7, 3.9, 1.4], 'Iris-versicolor'), ([5.0, 3.4, 1.5, 0.2], 'Iris-setosa'), ([5.7, 3.8, 1.7, 0.3], 'Iris-setosa'), ([6.5, 2.8, 4.6, 1.5], 'Iris-versicolor'), ([5.1, 2.5, 3.0, 1.1], 'Iris-versicolor'), ([6.5, 3.0, 5.8, 2.2], 'Iris-virginica')]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Layer' object has no attribute 'softmax'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-358-9dfc2275bc6f>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-349-15881fbbd735>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-349-15881fbbd735>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-348-9c1c523c6b96>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_output_layer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mneuron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mneuron\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mneuron_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mneuron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mneuron\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Layer' object has no attribute 'softmax'"
          ]
        }
      ]
    }
  ]
}