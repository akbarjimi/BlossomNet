{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tWngIP5CvyU7H90KXCeYbnI3H7fM6e3c",
      "authorship_tag": "ABX9TyP9qxXlh2rBYTPmZkCGJDB6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akbarjimi/BlossomNet/blob/main/Iris_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load Dataset ðŸ”„"
      ],
      "metadata": {
        "id": "lF8gLedNdQv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import csv\n",
        "from typing import List, Tuple, Any\n",
        "import math\n",
        "import pickle"
      ],
      "metadata": {
        "id": "y5hCJB4_FeVh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "d-y8bz9OCL-A"
      },
      "outputs": [],
      "source": [
        "def load_dataset(file_path: str) -> Tuple[List[List[float]], List[Any]]:\n",
        "    \"\"\"\n",
        "    Loads the dataset from a CSV file.\n",
        "\n",
        "    This function assumes that the CSV file has a header row and that:\n",
        "    - All columns except the last one are features (converted to float).\n",
        "    - The last column is the label (left as a string; convert if needed).\n",
        "\n",
        "    Note:\n",
        "      The current implementation is particularly suited for datasets like the Iris dataset.\n",
        "      For other datasets, you might want to modify the logic (e.g., to change the label column index).\n",
        "\n",
        "    Parameters:\n",
        "      file_path (str): Path to the dataset file.\n",
        "\n",
        "    Returns:\n",
        "      Tuple[List[List[float]], List[Any]]:\n",
        "          - data: A list of rows, each row is a list of features as floats.\n",
        "          - labels: A list of labels corresponding to each row.\n",
        "    \"\"\"\n",
        "    data: List[List[float]] = []\n",
        "    labels: List[Any] = []\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            reader = csv.reader(file)\n",
        "            header = next(reader, None)  # Skip header row if exists\n",
        "\n",
        "            # Process each row in the CSV file\n",
        "            for row in reader:\n",
        "                if row:  # Ensure the row is not empty\n",
        "                    # Convert all columns except the last one into floats (features)\n",
        "                    try:\n",
        "                        features = [float(item) for item in row[:-1]]\n",
        "                    except ValueError as ve:\n",
        "                        print(f\"Could not convert features to float in row: {row}. Error: {ve}\")\n",
        "                        continue\n",
        "                    # The last column is considered the label (remains as string or processed further)\n",
        "                    label = row[-1]\n",
        "\n",
        "                    data.append(features)\n",
        "                    labels.append(label)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the file: {e}\")\n",
        "\n",
        "    return data, labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    dataset, labels = load_dataset('/content/drive/MyDrive/Colab Notebooks/Iris Classification/Iris.csv')\n",
        "    print(dataset[:3],\"\\n\")\n",
        "    print(labels[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5moBn_B-ewV",
        "outputId": "7b00749c-e259-4711-82c7-5533e00ccf84"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0, 5.1, 3.5, 1.4, 0.2], [2.0, 4.9, 3.0, 1.4, 0.2], [3.0, 4.7, 3.2, 1.3, 0.2]] \n",
            "\n",
            "['Iris-setosa', 'Iris-setosa', 'Iris-setosa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_dataset(dataset: List[List[Any]]) -> bool:\n",
        "    \"\"\"\n",
        "    Ø¨Ø±Ø±Ø³ÛŒ ØµØ­Øª Ø³Ø§Ø®ØªØ§Ø± Ùˆ ÛŒÚ©Ù¾Ø§Ø±Ú†Ú¯ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡.\n",
        "\n",
        "    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡:\n",
        "      - Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù„ÛŒ Ù†Ø¨Ø§Ø´Ø¯.\n",
        "      - ØªÙ…Ø§Ù…ÛŒ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¯Ø§Ø±Ø§ÛŒ ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒ ÛŒÚ©Ø³Ø§Ù† Ø¨Ø§Ø´Ù†Ø¯.\n",
        "\n",
        "    Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:\n",
        "      dataset (List[List[Any]]): Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡Ø› Ù‡Ø± Ø±Ø¯ÛŒÙ Ù†ÛŒØ² Ù„ÛŒØ³ØªÛŒ Ø§Ø² ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§Ø³Øª.\n",
        "\n",
        "    Ø®Ø±ÙˆØ¬ÛŒ:\n",
        "      bool: Ø§Ú¯Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ ØµØ­ÛŒØ­ Ø¨Ø§Ø´Ø¯ØŒ Ù…Ù‚Ø¯Ø§Ø± True Ùˆ Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±Øª False Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.\n",
        "    \"\"\"\n",
        "    # Ø§Ú¯Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù„ÛŒ Ø¨Ø§Ø´Ø¯ØŒ Ù…Ø¹ØªØ¨Ø± Ù†ÛŒØ³Øª.\n",
        "    if not dataset:\n",
        "        return False\n",
        "\n",
        "    # ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ø§ÙˆÙ„ÛŒÙ† Ø±Ø¯ÛŒÙ Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…Ø¨Ù†Ø§ Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ….\n",
        "    num_features = len(dataset[0])\n",
        "    for row in dataset:\n",
        "        # Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ù‡Ø± Ø±Ø¯ÛŒÙ Ù‡Ù…Ø§Ù† ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø§ Ø±Ø¯ÛŒÙ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.\n",
        "        if len(row) != num_features:\n",
        "            return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "br0ElRjWdxiw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² ÛŒÚ© Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ù…Ø¹ØªØ¨Ø± (Ù…Ø«Ù„Ø§Ù‹ Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØªØ§Ø³Øª Iris ÛŒØ§ Ø³Ø§ÛŒØ± Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§)\n",
        "    valid_dataset = [\n",
        "        [5.1, 3.5, 1.4, 0.2],\n",
        "        [4.9, 3.0, 1.4, 0.2],\n",
        "        [6.2, 3.4, 5.4, 2.3]\n",
        "    ]\n",
        "\n",
        "    # Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² ÛŒÚ© Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ù†Ø§Ù…Ø¹ØªØ¨Ø± (ÛŒÚ© Ø±Ø¯ÛŒÙ Ø¨Ø§ ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒ Ù…ØªÙØ§ÙˆØª)\n",
        "    invalid_dataset = [\n",
        "        [5.1, 3.5, 1.4, 0.2],\n",
        "        [4.9, 3.0, 1.4],  # Ø§ÛŒÙ† Ø±Ø¯ÛŒÙ ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒ Ú©Ù…ØªØ±ÛŒ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø¨Ù‚ÛŒÙ‡ Ø¯Ø§Ø±Ø¯.\n",
        "        [6.2, 3.4, 5.4, 2.3]\n",
        "    ]\n",
        "\n",
        "    # Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ Ùˆ Ú†Ø§Ù¾ Ù†ØªØ§ÛŒØ¬\n",
        "    print(\"Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ù…Ø¹ØªØ¨Ø±:\", validate_dataset(valid_dataset))    # Ø®Ø±ÙˆØ¬ÛŒ: True\n",
        "    print(\"Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ù†Ø§Ù…Ø¹ØªØ¨Ø±:\", validate_dataset(invalid_dataset))  # Ø®Ø±ÙˆØ¬ÛŒ: False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuwqFF-WEiB9",
        "outputId": "24f891a0-c11d-4784-a40a-895ddd99ca33"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ù…Ø¹ØªØ¨Ø±: True\n",
            "Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ù†Ø§Ù…Ø¹ØªØ¨Ø±: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_data_loading(file_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Ø¢Ø²Ù…ÙˆÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯ ØªØ§Ø¨Ø¹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡.\n",
        "\n",
        "    ÙˆØ±ÙˆØ¯ÛŒ:\n",
        "      file_path (str): Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ CSV Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§.\n",
        "\n",
        "    Ø®Ø±ÙˆØ¬ÛŒ:\n",
        "      None: Ù†ØªÛŒØ¬Ù‡ ØªØ³Øª Ø§Ø² Ø·Ø±ÛŒÙ‚ Ú†Ø§Ù¾ Ù¾ÛŒØ§Ù… Ø¨Ù‡ Ú©Ù†Ø³ÙˆÙ„ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
        "\n",
        "    ØªÙˆØ¶ÛŒØ­Ø§Øª:\n",
        "      Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø§Ø¨ØªØ¯Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø² ÙØ§ÛŒÙ„ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² load_dataset Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯.\n",
        "      Ø³Ù¾Ø³ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² validate_dataset Ø§Ø¹ØªØ¨Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
        "      Ø¯Ø± ØµÙˆØ±Øª Ù…ÙˆÙÙ‚ÛŒØªØŒ Ù¾ÛŒØ§Ù… Ù…ÙˆÙÙ‚ÛŒØª Ø±Ø§ Ú†Ø§Ù¾ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø¯Ø± ØµÙˆØ±Øª Ø¨Ø±ÙˆØ² Ø®Ø·Ø§ØŒ Ù¾ÛŒØºØ§Ù… Ù…Ù†Ø§Ø³Ø¨ Ø±Ø§ Ú†Ø§Ù¾ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        dataset, labels = load_dataset(file_path)\n",
        "        assert validate_dataset(dataset), \"Dataset validation failed.\"\n",
        "        print(\"Data loaded and validated successfully.\")\n",
        "    except AssertionError as error:\n",
        "        print(f\"Test failed: {error}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"File not found. Make sure the dataset file exists at the specified path.\")\n",
        "    except Exception as error:\n",
        "        print(f\"An error occurred: {error}\")"
      ],
      "metadata": {
        "id": "mKGpyOkmd8ke"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_loading('/content/drive/MyDrive/Colab Notebooks/Iris Classification/Iris.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "085zceueHOJl",
        "outputId": "f1295b1c-88e6-4667-c4c0-f91462f761d8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded and validated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Normalize the Data ðŸ”„"
      ],
      "metadata": {
        "id": "PBavN6UwmFU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transpose(data: List[List]) -> List[List]:\n",
        "    \"\"\"\n",
        "    ØªØ±Ø§Ù†Ù‡Ø§Ø¯Ù‡ Ú©Ø±Ø¯Ù† ÛŒÚ© Ù„ÛŒØ³Øª Ø¯ÙˆØ¨Ø¹Ø¯ÛŒ.\n",
        "\n",
        "    Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:\n",
        "        data (List[List]): Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡â€ŒØµÙˆØ±Øª Ù„ÛŒØ³Øª Ø§Ø² Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ (Ù…Ø«Ù„Ø§Ù‹ Ù…Ø§ØªØ±ÛŒØ³)\n",
        "\n",
        "    Ø®Ø±ÙˆØ¬ÛŒ:\n",
        "        List[List]: Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ±Ø§Ù†Ù‡Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡\n",
        "    \"\"\"\n",
        "    # Ø¹Ù„Ø§Ù…Øª Ø³ØªØ§Ø±Ù‡ Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ Ø±Ø§ Ø¢Ù†Ù¾Ú© Ù…ÛŒÚ©Ù†Ø¯\n",
        "    # ØªØ§Ø¨Ø¹ Ø²ÛŒÙ¾ Ø³ØªÙˆÙ† n Ø§Ø² Ù‡Ø± Ø³Ø·Ø± Ø±Ø§ Ú©Ù†Ø§Ø± Ù‡Ù… Ù‚Ø±Ø§Ø± Ù…ÛŒØ¯Ù‡Ø¯ Ùˆ Ø¨Ø¯ÛŒÙ† ÙˆØ³ÛŒÙ„Ù‡ Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ Ø±Ø§ ØªØ±Ø§Ù†Ø³Ù¾ÙˆØ²Ù‡ Ù…ÛŒÚ©Ù†Ø¯.\n",
        "    return [list(row) for row in zip(*data)]"
      ],
      "metadata": {
        "id": "ILbm5BhKp2fJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def min_max_normalizer(data: List[List[float]]) -> List[List[float]]:\n",
        "    \"\"\"\n",
        "    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø±Ùˆ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ´ Min-Max Ù†Ø±Ù…Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ù‡.\n",
        "    ÛŒØ¹Ù†ÛŒ Ù‡Ù…Ù‡â€ŒÛŒ Ø§Ø¹Ø¯Ø§Ø¯ Ø±Ùˆ Ø¨ÛŒÙ† 0 Ùˆ 1 Ù…ÛŒØ§Ø±Ù‡ØŒ ØªØ§ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡â€ŒØªØ± Ø¨Ø´Ù‡.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ú†Ø±Ø®ÙˆÙ†Ø¯Ù† Ù…Ø§ØªØ±ÛŒØ³ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø³ØªÙˆÙ†ÛŒ\n",
        "    transposed_data = transpose(data)\n",
        "\n",
        "    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…ÛŒÙ†ÛŒÙ…Ù… Ùˆ Ù…Ø§Ú©Ø²ÛŒÙ…Ù… Ù‡Ø± Ø³ØªÙˆÙ†\n",
        "    min_vals = [min(col) for col in transposed_data]\n",
        "    max_vals = [max(col) for col in transposed_data]\n",
        "\n",
        "    scaled_data = []\n",
        "\n",
        "    # Ù†Ø±Ù…Ø§Ù„ Ú©Ø±Ø¯Ù† Ù‡Ø± Ù…Ù‚Ø¯Ø§Ø± Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙØ±Ù…ÙˆÙ„ min-max\n",
        "    for row_index, row in enumerate(data):\n",
        "        scaled_row = []\n",
        "        for val, min_val, max_val in zip(row, min_vals, max_vals):\n",
        "            range_val = max_val - min_val\n",
        "            if range_val == 0:\n",
        "                scaled_row.append(0.0)  # Ø§Ú¯Ø± Ù‡Ù…Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨Ø±Ø§Ø¨Ø± Ø¨ÙˆØ¯Ù†ØŒ Ø®Ø±ÙˆØ¬ÛŒ Ø±Ùˆ ØµÙØ± Ø¨Ø²Ø§Ø±\n",
        "            else:\n",
        "                scaled_val = (val - min_val) / range_val\n",
        "                scaled_row.append(scaled_val)\n",
        "        scaled_data.append(scaled_row)\n",
        "\n",
        "    return scaled_data\n"
      ],
      "metadata": {
        "id": "GHEPq3Qnm84C"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(dataset: List[List[float]], training_size: float = 0.7, validation_size: float = 0.15) -> Tuple[List[List[float]], List[List[float]], List[List[float]]]:\n",
        "    \"\"\"\n",
        "    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¯ÛŒØªØ§Ø³Øª Ø±Ùˆ Ø¨Ù‡ Ø³Ù‡ Ø¨Ø®Ø´ ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†Ù‡: Ø¢Ù…ÙˆØ²Ø´ØŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ùˆ ØªØ³Øª.\n",
        "\n",
        "    Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:\n",
        "    dataset (list of lists): Ø¯ÛŒØªØ§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒÙ… ØªÙ‚Ø³ÛŒÙ…Ø´ Ú©Ù†ÛŒÙ….\n",
        "    training_size (float): Ø¯Ø±ØµØ¯ÛŒ Ø§Ø² Ø¯ÛŒØªØ§ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒØ´Ù‡ (Ù…Ø«Ù„Ø§Ù‹ Û°.Û· ÛŒØ¹Ù†ÛŒ Û·Û°Ùª).\n",
        "    validation_size (float): Ø¯Ø±ØµØ¯ÛŒ Ø§Ø² Ø¯ÛŒØªØ§ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒØ´Ù‡ (Ù…Ø«Ù„Ø§Ù‹ Û°.Û±Ûµ ÛŒØ¹Ù†ÛŒ Û±ÛµÙª).\n",
        "\n",
        "    Ø®Ø±ÙˆØ¬ÛŒ:\n",
        "    ÛŒÙ‡ ØªØ§Ù¾Ù„ Ø´Ø§Ù…Ù„ Ø³Ù‡ ØªØ§ Ù„ÛŒØ³ØªÙ‡: Ø¯ÛŒØªØ§ÛŒ Ø¢Ù…ÙˆØ²Ø´ØŒ Ø¯ÛŒØªØ§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒØŒ Ø¯ÛŒØªØ§ÛŒ ØªØ³Øª.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ø§ÙˆÙ„ Ø¯ÛŒØªØ§ Ø±Ùˆ Ø¨Ù‡ ØµÙˆØ±Øª ØªØµØ§Ø¯ÙÛŒ Ù‚Ø§Ø·ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ ØªØ±ØªÛŒØ¨Ø´ ØªØ£Ø«ÛŒØ± Ù†Ø°Ø§Ø±Ù‡\n",
        "    random.shuffle(dataset)\n",
        "\n",
        "    # Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ù„ Ø¯ÛŒØªØ§\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø±Ø² Ø¨ÛŒÙ† Ø¨Ø®Ø´â€ŒÙ‡Ø§\n",
        "    train_end = int(training_size * total_size)\n",
        "    val_end = int((training_size + validation_size) * total_size)\n",
        "\n",
        "    # Ø¨Ø±Ø´ Ø¯Ø§Ø¯Ù† Ø¯ÛŒØªØ§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø±ØµØ¯Ù‡Ø§\n",
        "    training_data = dataset[:train_end]\n",
        "    validation_data = dataset[train_end:val_end]\n",
        "    test_data = dataset[val_end:]\n",
        "\n",
        "    return training_data, validation_data, test_data\n"
      ],
      "metadata": {
        "id": "JUk8uAB0k1F1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    [5.1, 3.5, 1.4, 0.2],\n",
        "    [4.9, 3.0, 1.4, 0.2],\n",
        "    [6.2, 3.4, 5.4, 2.3],\n",
        "    [5.9, 3.0, 5.1, 1.8],\n",
        "    [5.4, 3.9, 1.7, 0.4],\n",
        "    [6.7, 3.1, 4.7, 1.5],\n",
        "    [5.6, 2.8, 4.9, 2.0],\n",
        "    [5.7, 2.1, 4.0, 2.1],\n",
        "    [5.8, 2.2, 9.4, 1.2],\n",
        "    [5.9, 2.3, 5.5, 0.8],\n",
        "]\n",
        "\n",
        "train, val, test = split_data(data, training_size=0.5, validation_size=0.2)\n",
        "print(\"Train:\", train)\n",
        "print(\"Validation:\", val)\n",
        "print(\"Test:\", test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vsrx7eIk6nIH",
        "outputId": "43f08fec-3cfe-49de-9070-4baf2f516407"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: [[5.1, 3.5, 1.4, 0.2], [4.9, 3.0, 1.4, 0.2], [6.2, 3.4, 5.4, 2.3], [5.7, 2.1, 4.0, 2.1], [5.6, 2.8, 4.9, 2.0]]\n",
            "Validation: [[5.9, 2.3, 5.5, 0.8], [5.9, 3.0, 5.1, 1.8]]\n",
            "Test: [[5.4, 3.9, 1.7, 0.4], [5.8, 2.2, 9.4, 1.2], [6.7, 3.1, 4.7, 1.5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_split_data():\n",
        "    \"\"\"\n",
        "    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù‡ Ú©Ù‡ ØªØ§Ø¨Ø¹ split_data Ø¯Ø±Ø³Øª Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù‡ ÛŒØ§ Ù†Ù‡.\n",
        "    ØªÙˆÛŒ ØªØ³ØªØŒ Ø§ÙˆÙ„ Ø¯ÛŒØªØ§Ø³Øª Ø±Ùˆ Ù„ÙˆØ¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…ØŒ Ø¨Ø¹Ø¯ Ù†Ø±Ù…Ø§Ù„Ø§ÛŒØ²Ø´ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…ØŒ Ø¨Ø¹Ø¯ ØªÙ‚Ø³ÛŒÙ…Ø´ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….\n",
        "    Ø¨Ø¹Ø¯Ø´ Ú†Ú© Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ù‡Ø± Ø¨Ø®Ø´ Ø§Ø² Ø¯ÛŒØªØ§ Ø®Ø§Ù„ÛŒ Ù†Ø¨Ø§Ø´Ù‡ Ùˆ Ù…Ø¬Ù…ÙˆØ¹Ø´ÙˆÙ† Ù‡Ù… Ø¨Ø±Ø§Ø¨Ø± Ø¨Ø§ Ø¯ÛŒØªØ§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø§Ø´Ù‡.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø¯ÛŒØªØ§Ø³Øª\n",
        "        file_path = '/content/drive/MyDrive/Colab Notebooks/Iris Classification/Iris.csv'\n",
        "\n",
        "        # Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¯ÛŒØªØ§ Ùˆ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§\n",
        "        dataset, labels = load_dataset(file_path)\n",
        "\n",
        "        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒØªØ§ Ø¨Ø§ Min-Max\n",
        "        dataset = min_max_normalizer(dataset)\n",
        "\n",
        "        # ØªÙ‚Ø³ÛŒÙ… Ø¯ÛŒØªØ§ Ø¨Ù‡ Ø³Ù‡ Ø¨Ø®Ø´\n",
        "        training_data, validation_data, test_data = split_data(dataset)\n",
        "\n",
        "        # ØªØ³Øª Ø§ÛŒÙ†Ú©Ù‡ Ù‡ÛŒÚ†â€ŒÚ©Ø¯ÙˆÙ… Ø§Ø² Ø¨Ø®Ø´â€ŒÙ‡Ø§ Ø®Ø§Ù„ÛŒ Ù†Ø¨Ø§Ø´Ù‡\n",
        "        assert len(training_data) > 0, \"Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø®Ø§Ù„ÛŒÙ‡.\"\n",
        "        assert len(validation_data) > 0, \"Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø®Ø§Ù„ÛŒÙ‡.\"\n",
        "        assert len(test_data) > 0, \"Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª Ø®Ø§Ù„ÛŒÙ‡.\"\n",
        "\n",
        "        # ØªØ³Øª Ø§ÛŒÙ†Ú©Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ØªØºÛŒÛŒØ± Ù†Ú©Ø±Ø¯Ù‡ Ø¨Ø§Ø´Ù‡\n",
        "        total_size = len(training_data) + len(validation_data) + len(test_data)\n",
        "        assert total_size == len(dataset), \"Ø®Ø·Ø§ Ø¯Ø± ØªÙ‚Ø³ÛŒÙ… Ø¯ÛŒØªØ§: ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Ø§ÙˆÙ„ÛŒÙ‡ Ù†Ù…ÛŒâ€ŒØ®ÙˆÙ†Ù‡.\"\n",
        "\n",
        "        print(\"âœ… ØªØ³Øª ØªÙ‚Ø³ÛŒÙ… Ø¯ÛŒØªØ§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.\")\n",
        "\n",
        "    except AssertionError as error:\n",
        "        print(f\"âŒ ØªØ³Øª Ø´Ú©Ø³Øª Ø®ÙˆØ±Ø¯: {error}\")\n",
        "\n",
        "    except Exception as error:\n",
        "        print(f\"âš ï¸ ÛŒÙ‡ Ø®Ø·Ø§ÛŒÛŒ Ù¾ÛŒØ´ Ø§ÙˆÙ…Ø¯: {error}\")\n"
      ],
      "metadata": {
        "id": "EH-SJwTEk4PT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_split_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH4BxNlN7rjY",
        "outputId": "3d19b5a2-7a11-4292-f980-47c64277fe05"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ØªØ³Øª ØªÙ‚Ø³ÛŒÙ… Ø¯ÛŒØªØ§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Define the Architecture ðŸ—"
      ],
      "metadata": {
        "id": "VUV-OoGf9CoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron:\n",
        "    \"\"\"Ú©Ù„Ø§Ø³ ÛŒÚ© Ù†ÙˆØ±ÙˆÙ† Ø³Ø§Ø¯Ù‡ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø®Ø±ÙˆØ¬ÛŒØŒ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ùˆ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\"\"\"\n",
        "\n",
        "    def __init__(self, weights: List[float], bias: float = None):\n",
        "        \"\"\"\n",
        "        Ù†ÙˆØ±ÙˆÙ† Ø¨Ø§ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ùˆ Ø¨Ø§ÛŒØ§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
        "\n",
        "        Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§:\n",
        "        weights: Ù„ÛŒØ³ØªÛŒ Ø§Ø² ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†ÙˆØ±ÙˆÙ†.\n",
        "        bias: Ù…Ù‚Ø¯Ø§Ø± Ø¨Ø§ÛŒØ§Ø³ Ø¨Ø±Ø§ÛŒ Ù†ÙˆØ±ÙˆÙ†. Ø§Ú¯Ø± Ø¯Ø§Ø¯Ù‡ Ù†Ø´ÙˆØ¯ØŒ ÛŒÚ© Ù…Ù‚Ø¯Ø§Ø± ØªØµØ§Ø¯ÙÛŒ Ø¨ÛŒÙ† -0.1 Ùˆ 0.1 Ø¨Ù‡ Ø¢Ù† Ø§Ø®ØªØµØ§Øµ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯.\n",
        "        \"\"\"\n",
        "        self.weights = weights\n",
        "        self.bias = bias if bias is not None else random.uniform(-0.1, 0.1)\n",
        "        self.inputs = []  # ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†ÙˆØ±ÙˆÙ† Ø±Ø§ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯\n",
        "        self.output = 0   # Ø®Ø±ÙˆØ¬ÛŒ Ù†ÙˆØ±ÙˆÙ† Ø±Ø§ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯\n",
        "\n",
        "    def forward(self, inputs: List[float]) -> float:\n",
        "        \"\"\"\n",
        "        Ø®Ø±ÙˆØ¬ÛŒ Ù†ÙˆØ±ÙˆÙ† Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ùˆ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
        "\n",
        "        Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§:\n",
        "        inputs: Ù„ÛŒØ³ØªÛŒ Ø§Ø² ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ Ù†ÙˆØ±ÙˆÙ†.\n",
        "\n",
        "        Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯:\n",
        "        Ø®Ø±ÙˆØ¬ÛŒ Ù†ÙˆØ±ÙˆÙ†.\n",
        "        \"\"\"\n",
        "        self.inputs = inputs  # Ø°Ø®ÛŒØ±Ù‡ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ù…Ø±Ø§Ø­Ù„ Ø¨Ø¹Ø¯ÛŒ\n",
        "        weighted_sum = sum([input_ * weight for input_, weight in zip(inputs, self.weights)])\n",
        "        self.output = weighted_sum + self.bias  # Ø¬Ù…Ø¹ Ú©Ø±Ø¯Ù† ÙˆØ²Ù†â€ŒÙ‡Ø§ Ùˆ Ø¨Ø§ÛŒØ§Ø³\n",
        "        return self.output\n",
        "\n",
        "    def activation(self, output: float) -> float:\n",
        "        \"\"\"\n",
        "        ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ù†ÙˆØ±ÙˆÙ† Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
        "\n",
        "        Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ØŒ Ø¨Ù‡ Ø·ÙˆØ± Ø³Ø§Ø¯Ù‡ Ø§Ø² ØªØ§Ø¨Ø¹ Ø³ÛŒÚ¯Ù…ÙˆÛŒØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….\n",
        "        \"\"\"\n",
        "\n",
        "        # Ø§Ø² ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒÚ¯Ù…ÙˆÛŒØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…\n",
        "        return 1 / (1 + (2.718 ** -output))  # Ø§ÛŒÙ† ÛŒØ¹Ù†ÛŒ Ø³ÛŒÚ¯Ù…ÙˆÛŒØ¯\n",
        "\n",
        "    def compute_gradient(self, delta: float) -> List[float]:\n",
        "        \"\"\"\n",
        "        Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ø¨Ø±Ø§ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ù„ØªØ§ (Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø·Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Ø¨Ø¹Ø¯ÛŒ) Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
        "\n",
        "        Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§:\n",
        "        delta: Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø·Ø§ Ø§Ø² Ù„Ø§ÛŒÙ‡ Ø¨Ø¹Ø¯ÛŒ.\n",
        "\n",
        "        Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯:\n",
        "        Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆØ²Ù†.\n",
        "        \"\"\"\n",
        "        gradients = [delta * input_ for input_ in self.inputs]  # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†â€ŒÙ‡Ø§\n",
        "        return gradients\n",
        "\n",
        "    def update_weights(self, learning_rate: float, gradients: List[float]):\n",
        "        \"\"\"\n",
        "        ÙˆØ²Ù†â€ŒÙ‡Ø§ Ùˆ Ø¨Ø§ÛŒØ§Ø³ Ù†ÙˆØ±ÙˆÙ† Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†â€ŒÙ‡Ø§ Ùˆ Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
        "\n",
        "        Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§:\n",
        "        learning_rate: Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§.\n",
        "        gradients: Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆØ²Ù†.\n",
        "        \"\"\"\n",
        "        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§\n",
        "        self.weights = [w - learning_rate * g for w, g in zip(self.weights, gradients)]\n",
        "        self.bias -= learning_rate * gradients[-1]  # Ø¨Ø§ÛŒØ§Ø³ Ø±Ø§ Ù†ÛŒØ² Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…\n",
        "\n",
        "    def propagate_error_back(self) -> List[float]:\n",
        "        \"\"\"\n",
        "        Ø§ÛŒÙ† Ù…ØªØ¯ Ø®Ø·Ø§ÛŒ Ù†ÙˆØ±ÙˆÙ† Ø±Ùˆ Ø¨Ø±Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Ù‚Ø¨Ù„ÛŒ Ø­Ø³Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†Ù‡.\n",
        "        ÛŒØ¹Ù†ÛŒ Ø§ÙˆÙ„ Ù…Ø´ØªÙ‚ Ø³ÛŒÚ¯Ù…ÙˆÛŒØ¯ Ø±Ùˆ Ø§Ø² Ø®Ø±ÙˆØ¬ÛŒ Ø®ÙˆØ¯Ø´ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù‡ØŒ\n",
        "        Ø¨Ø¹Ø¯ Ø¨Ø§ Ù‡Ø± ÙˆØ²Ù† Ø¶Ø±Ø¨ Ù…ÛŒâ€ŒÚ©Ù†Ù‡ ØªØ§ Ø¨Ú¯Ù‡ Ù‡Ø± ÙˆØ±ÙˆØ¯ÛŒ Ú†Ù‚Ø¯Ø± Ø¯Ø± Ø®Ø·Ø§ Ø³Ù‡Ù… Ø¯Ø§Ø±Ù‡.\n",
        "        \"\"\"\n",
        "        # Ù…Ø´ØªÙ‚ Ø³ÛŒÚ¯Ù…ÙˆÛŒØ¯: output * (1 - output)\n",
        "        deriv = self.output * (1 - self.output)\n",
        "        # Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆØ²Ù†ØŒ Ø³Ù‡Ù… Ø®Ø·Ø§ = Ù…Ø´ØªÙ‚ * ÙˆØ²Ù†\n",
        "        return [deriv * w for w in self.weights]"
      ],
      "metadata": {
        "id": "uJCkaPTT90u7"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ØªØ³Øª Ø¹Ù…Ù„Ú©Ø±Ø¯ Ú©Ù„Ø§Ø³ Ù†ÙˆØ±ÙˆÙ†\n",
        "\n",
        "# Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© Ù†ÙˆØ±ÙˆÙ† Ø¨Ø§ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ùˆ Ø¨Ø§ÛŒØ§Ø³ ØªØµØ§Ø¯ÙÛŒ\n",
        "weights = [random.uniform(-1, 1) for _ in range(3)]  # Ø³Ù‡ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø§ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ ØªØµØ§Ø¯ÙÛŒ\n",
        "bias = random.uniform(-0.1, 0.1)  # Ø¨Ø§ÛŒØ§Ø³ ØªØµØ§Ø¯ÙÛŒ\n",
        "neuron = Neuron(weights, bias)\n",
        "\n",
        "# ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù†ÙˆØ±ÙˆÙ†\n",
        "inputs = [0.5, 0.2, 0.8]  # Ø³Ù‡ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù†ÙˆØ±ÙˆÙ†\n",
        "\n",
        "# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø®Ø±ÙˆØ¬ÛŒ Ù†ÙˆØ±ÙˆÙ†\n",
        "output = neuron.forward(inputs)\n",
        "print(f\"Ø®Ø±ÙˆØ¬ÛŒ Ù†ÙˆØ±ÙˆÙ† Ù‚Ø¨Ù„ Ø§Ø² ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ: {output}\")\n",
        "\n",
        "# Ø§Ø¹Ù…Ø§Ù„ ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ (Ø³ÛŒÚ¯Ù…ÙˆÛŒØ¯)\n",
        "activated_output = neuron.activation(output)\n",
        "print(f\"Ø®Ø±ÙˆØ¬ÛŒ Ù†ÙˆØ±ÙˆÙ† Ø¨Ø¹Ø¯ Ø§Ø² ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ: {activated_output}\")\n",
        "\n",
        "# ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø·Ø§ (Ø¯Ù„ØªØ§) Ø¨Ø±Ø§Ø¨Ø± 0.1 Ø§Ø³Øª\n",
        "delta = 0.1\n",
        "gradients = neuron.compute_gradient(delta)\n",
        "print(f\"Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§: {gradients}\")\n",
        "\n",
        "# Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø¨Ø§ Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ 0.01\n",
        "neuron.update_weights(learning_rate=0.01, gradients=gradients)\n",
        "print(f\"ÙˆØ²Ù†â€ŒÙ‡Ø§ Ùˆ Ø¨Ø§ÛŒØ§Ø³ Ø¨Ø¹Ø¯ Ø§Ø² Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ: {neuron.weights}, {neuron.bias}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijZ-pMCIAScK",
        "outputId": "a4c604db-3287-4c3c-d258-1c657f1ed703"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ø®Ø±ÙˆØ¬ÛŒ Ù†ÙˆØ±ÙˆÙ† Ù‚Ø¨Ù„ Ø§Ø² ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ: -0.23141342851715974\n",
            "Ø®Ø±ÙˆØ¬ÛŒ Ù†ÙˆØ±ÙˆÙ† Ø¨Ø¹Ø¯ Ø§Ø² ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ: 0.4424093676931212\n",
            "Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§: [0.05, 0.020000000000000004, 0.08000000000000002]\n",
            "ÙˆØ²Ù†â€ŒÙ‡Ø§ Ùˆ Ø¨Ø§ÛŒØ§Ø³ Ø¨Ø¹Ø¯ Ø§Ø² Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ: [0.35225086873070327, -0.10548841036494608, -0.4501960971073645], -0.028014303123630516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "    \"\"\"ÛŒÙ‡ Ù„Ø§ÛŒÙ‡ ØªÙˆÛŒ Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ú©Ù‡ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ø±Ùˆ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù‡ØŒ Ù…ÛŒâ€ŒÙØ±Ø³ØªÙ‡ ØªÙˆÛŒ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ØŒ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù‡ Ùˆ Ø®Ø±ÙˆØ¬ÛŒ Ù…ÛŒâ€ŒØ³Ø§Ø²Ù‡.\"\"\"\n",
        "\n",
        "    def __init__(self, neurons: List[Neuron], is_output_layer: bool = False):\n",
        "        \"\"\"\n",
        "        Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§:\n",
        "          neurons: Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§ÛŒÙ† Ù„Ø§ÛŒÙ‡.\n",
        "          is_output_layer: Ø§Ú¯Ù‡ True Ø¨Ø§Ø´Ù‡ØŒ Ø§ÛŒÙ† Ù„Ø§ÛŒÙ‡ Ø¢Ø®Ø±Ù‡ Ùˆ Ø®Ø±ÙˆØ¬ÛŒâ€ŒØ´Ùˆ Ø¨Ø§ softmax Ù…ÛŒâ€ŒØ³Ø§Ø²Ù‡.\n",
        "        \"\"\"\n",
        "        self.neurons = neurons\n",
        "        self.is_output_layer = is_output_layer\n",
        "\n",
        "    def forward(self, inputs: List[float]) -> List[float]:\n",
        "        \"\"\"\n",
        "        Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ùˆ Ù…ÛŒâ€ŒÙØ±Ø³ØªÙ‡ ØªÙˆÛŒ Ù‡Ø± Ù†ÙˆØ±ÙˆÙ† Ùˆ Ø®Ø±ÙˆØ¬ÛŒ Ø®Ø§Ù… Ù‡Ø± Ù†ÙˆØ±ÙˆÙ† (logits) Ø±Ùˆ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù‡.\n",
        "        Ø¨Ø¹Ø¯ Ø§Ú¯Ù‡ Ù„Ø§ÛŒÙ‡ Ø¢Ø®Ø±ÛŒ Ø¨Ø§Ø´Ù‡ softmax Ù…ÛŒâ€ŒØ²Ù†Ù‡Ø› ÙˆÚ¯Ø±Ù†Ù‡ Ø¨Ø§ Ù…ØªØ¯ activation Ø®ÙˆØ¯ Ù†ÙˆØ±ÙˆÙ† Ø³ÛŒÚ¯Ù…ÙˆÛŒØ¯ Ù…ÛŒâ€ŒØ²Ù†Ù‡.\n",
        "\n",
        "        inputs: Ù„ÛŒØ³Øª ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ù‡ Ø§ÛŒÙ† Ù„Ø§ÛŒÙ‡ (Ù…Ø«Ù„Ø§Ù‹ Ø®Ø±ÙˆØ¬ÛŒ Ù„Ø§ÛŒÙ‡ Ù‚Ø¨Ù„ÛŒ).\n",
        "        Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯ÙˆÙ†Ù‡: Ù„ÛŒØ³Øª Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø§ÛŒÙ† Ù„Ø§ÛŒÙ‡.\n",
        "        \"\"\"\n",
        "        # logits ÛŒØ¹Ù†ÛŒ Â«Ø®Ø±ÙˆØ¬ÛŒ Ø®Ø§Ù… Ù†ÙˆØ±ÙˆÙ† Ù‚Ø¨Ù„ Ø§Ø² ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒÂ»\n",
        "        logits = [neuron.forward(inputs) for neuron in self.neurons]\n",
        "\n",
        "        if self.is_output_layer:\n",
        "            # Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø³ØªÛŒ softmax: exp Ù‡Ø± Ø¹Ø¯Ø¯ / Ù…Ø¬Ù…ÙˆØ¹ expÙ‡Ø§\n",
        "            exp_vals = [math.exp(x) for x in logits]\n",
        "            sum_exp = sum(exp_vals)\n",
        "            return [v / sum_exp for v in exp_vals]\n",
        "        else:\n",
        "            # Ø¨Ø±Ø§ÛŒ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù…ÛŒØ§Ù†ÛŒ: Ù‡Ø± Ù†ÙˆØ±ÙˆÙ† Ø®ÙˆØ¯Ø´ Ù…ØªØ¯ activation Ø¯Ø§Ø±Ù‡\n",
        "            return [neuron.activation(x) for neuron, x in zip(self.neurons, logits)]\n",
        "\n",
        "    def backward(self, delta: List[float], learning_rate: float) -> List[int]:\n",
        "        \"\"\"\n",
        "        Ø¯Ù„ØªØ§ (Ø®Ø·Ø§) Ø§Ø² Ù„Ø§ÛŒÙ‡ Ø¨Ø¹Ø¯ÛŒ Ø±Ùˆ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù‡ Ùˆ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†ÙˆØ±ÙˆÙ†:\n",
        "        1. Ø¨Ø§ compute_gradient Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†â€ŒÙ‡Ø§ Ø±Ùˆ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡\n",
        "        2. Ø¨Ø§ update_weights ÙˆØ²Ù† Ùˆ Ø¨Ø§ÛŒØ§Ø³ Ø±Ùˆ Ø¢Ù¾Ø¯ÛŒØª Ù…ÛŒâ€ŒÚ©Ù†Ù‡\n",
        "        3. Ø¨Ø§ propagate_error_back Ø¯Ù„ØªØ§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ø±Ùˆ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ù‡\n",
        "        Ø¯Ø± Ù†Ù‡Ø§ÛŒØª Ù‡Ù…Ù‡ Ø¯Ù„ØªØ§Ù‡Ø§ Ø±Ùˆ Ø¬Ù…Ø¹ Ù…ÛŒâ€ŒÚ©Ù†Ù‡ Ùˆ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯ÙˆÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Ù‚Ø¨Ù„ÛŒ.\n",
        "\n",
        "        delta: Ù„ÛŒØ³Øª Ø®Ø·Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†ÙˆØ±ÙˆÙ† Ø§ÛŒÙ† Ù„Ø§ÛŒÙ‡\n",
        "        learning_rate: Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ\n",
        "        Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯ÙˆÙ†Ù‡: Ù„ÛŒØ³Øª delta Ø¨Ø±Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Ù‚Ø¨Ù„ÛŒ\n",
        "        \"\"\"\n",
        "        propagated = []  # Ø§ÛŒÙ† Ù„ÛŒØ³ØªØŒ Ù‡Ø± Ø¹Ù†ØµØ±Ø´ Ù„ÛŒØ³Øª Ø®Ø·Ø§Ù‡Ø§ÛŒÛŒâ€ŒÛŒÙ‡ Ú©Ù‡ Ù‡Ø± Ù†ÙˆØ±ÙˆÙ† Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ù‡\n",
        "\n",
        "        for i, neuron in enumerate(self.neurons):\n",
        "            # 1. Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ø±Ùˆ Ø®ÙˆØ¯ Ù†ÙˆØ±ÙˆÙ† Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡\n",
        "            grads = neuron.compute_gradient(delta[i])\n",
        "\n",
        "            # 2. Ø®ÙˆØ¯ Ù†ÙˆØ±ÙˆÙ† ÙˆØ²Ù†â€ŒÙ‡Ø§ Ùˆ Ø¨Ø§ÛŒØ§Ø³ Ø±Ùˆ Ø¢Ù¾Ø¯ÛŒØª Ù…ÛŒâ€ŒÚ©Ù†Ù‡\n",
        "            neuron.update_weights(learning_rate, grads)\n",
        "\n",
        "            # 3. Ø®Ø·Ø§ Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†ÙˆØ±ÙˆÙ† (Ø¨Ø±Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Ù‚Ø¨Ù„ÛŒ)\n",
        "            propagated.append(neuron.propagate_error_back())\n",
        "\n",
        "        # Ø¬Ù…Ø¹ Ú©Ø±Ø¯Ù† Ø¯Ù„ØªØ§Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆØ±ÙˆØ¯ÛŒ\n",
        "        # zip(*propagated) Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ Ø±Ùˆ Ø³ØªÙˆÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù‡ ØªØ§ Ø¨ØªÙˆÙ†ÛŒÙ… Ø¬Ù…Ø¹ Ú©Ù†ÛŒÙ…\n",
        "        prev_delta = [sum(x) for x in zip(*propagated)]\n",
        "        return prev_delta"
      ],
      "metadata": {
        "id": "JBJr0gtOE391"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neurons = [\n",
        "    Neuron(weights=[0.5, -0.4], bias=0.1),\n",
        "    Neuron(weights=[-1.0, 2.0], bias=0.2)\n",
        "]\n",
        "layer = Layer(neurons, is_output_layer=False)\n",
        "\n",
        "layer_output = layer.forward(inputs)\n",
        "print(\"Ø®Ø±ÙˆØ¬ÛŒ Ù„Ø§ÛŒÙ‡:\", layer_output)\n",
        "\n",
        "delta = [0.1, -0.2]\n",
        "prev_delta = layer.backward(delta, learning_rate=0.01)\n",
        "print(\"Ø¯Ù„ØªØ§ Ø¨Ø±Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Ù‚Ø¨Ù„ÛŒ:\", prev_delta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1y_V3HZ-PPVD",
        "outputId": "73adbfef-7dbb-4e98-a76d-e26190c2f26a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ø®Ø±ÙˆØ¬ÛŒ Ù„Ø§ÛŒÙ‡: [0.5670860322814533, 0.5249766018409759]\n",
            "Ø¯Ù„ØªØ§ Ø¨Ø±Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Ù‚Ø¨Ù„ÛŒ: [0.008541449999999978, 0.10115658000000005]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    \"\"\"A neural network with multiple layers.\"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, layers: List[Layer], epochs: int, learning_rate: float):\n",
        "        self.layers = layers\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def forward(self, inputs: List[float]) -> List[float]:\n",
        "        \"\"\"Propagates inputs through the entire network.\n",
        "\n",
        "        Args:\n",
        "            inputs: A list of inputs to the network.\n",
        "\n",
        "        Returns:\n",
        "            A list of outputs from the final layer.\n",
        "        \"\"\"\n",
        "        outputs = inputs\n",
        "        for layer in self.layers:\n",
        "            outputs = layer.forward(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def backward(self, targets: List[float], outputs: List[float]):\n",
        "        \"\"\"Performs backpropagation to update the network's weights and biases.\n",
        "\n",
        "        Args:\n",
        "            targets: A list of target outputs.\n",
        "            outputs: A list of outputs from the final forward pass.\n",
        "        \"\"\"\n",
        "        delta = self.loss_derivative(outputs, targets)\n",
        "\n",
        "        for layer in reversed(self.layers):\n",
        "            delta = layer.backward(delta, self.learning_rate)\n",
        "\n",
        "    def compute_loss(self, predicted: List[float], actual: List[float]) -> float:\n",
        "        \"\"\"Calculates the loss for the predictions.\"\"\"\n",
        "        return LossFunction.cross_entropy(predicted, actual)\n",
        "\n",
        "    def loss_derivative(self, outputs: List[float], targets: List[float]) -> List[float]:\n",
        "        \"\"\"Computes the derivative of the loss function.\"\"\"\n",
        "        return [pred - target for pred, target in zip(outputs, targets)]\n",
        "\n",
        "    def train(self, training_data: List[tuple]):\n",
        "        \"\"\"Trains the network on the given data without mini-batches.\n",
        "\n",
        "        Args:\n",
        "            training_data: A list of tuples containing input and target data.\n",
        "        \"\"\"\n",
        "        num_samples = len(training_data)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            total_loss = 0\n",
        "            random.shuffle(training_data)\n",
        "\n",
        "            for inputs, targets in training_data:\n",
        "                outputs = self.forward(inputs)\n",
        "\n",
        "                loss = self.compute_loss(outputs, targets)\n",
        "                total_loss += loss\n",
        "\n",
        "                self.backward(targets, outputs)\n",
        "\n",
        "            avg_loss = total_loss / num_samples\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{self.epochs} complete. Average loss: {avg_loss:.4f}\")\n",
        "\n",
        "    def evaluate(self, test_data: List[tuple]) -> float:\n",
        "        \"\"\"Evaluates the network on the test data.\"\"\"\n",
        "        inputs_batch, targets_batch = zip(*test_data)\n",
        "        predictions = [self.forward(inputs) for inputs in inputs_batch]\n",
        "        accuracy = self.calculate_accuracy(predictions, targets_batch)\n",
        "        return accuracy\n",
        "\n",
        "    def predict(self, new_data: List[float]) -> List[float]:\n",
        "        \"\"\"Predicts the output for new input data.\n",
        "\n",
        "        Args:\n",
        "            new_data: A list of new input data.\n",
        "\n",
        "        Returns:\n",
        "            A list of predicted outputs.\n",
        "        \"\"\"\n",
        "        return self.forward(new_data)\n",
        "\n",
        "    def calculate_accuracy(self, predictions: List[List[float]], targets: List[List[float]]) -> float:\n",
        "        \"\"\"Calculates the accuracy of the model.\"\"\"\n",
        "        correct_predictions = 0\n",
        "        for pred, target in zip(predictions, targets):\n",
        "            predicted_class = np.argmax(pred)\n",
        "            true_class = np.argmax(target)\n",
        "            if predicted_class == true_class:\n",
        "                correct_predictions += 1\n",
        "        return correct_predictions / len(targets)\n",
        "\n",
        "    def save_weights(self, filename: str):\n",
        "        \"\"\"Saves the weights of the network to a file.\"\"\"\n",
        "        weights = [[neuron.weights for neuron in layer.neurons] for layer in self.layers]\n",
        "        biases = [[neuron.bias for neuron in layer.neurons] for layer in self.layers]\n",
        "\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump((weights, biases), f)\n",
        "\n",
        "    def load_weights(self, filename: str):\n",
        "        \"\"\"Loads weights into the network from a file.\"\"\"\n",
        "        with open(filename, 'rb') as f:\n",
        "            weights, biases = pickle.load(f)\n",
        "\n",
        "        for layer, layer_weights, layer_biases in zip(self.layers, weights, biases):\n",
        "            for neuron, neuron_weights, neuron_bias in zip(layer.neurons, layer_weights, layer_biases):\n",
        "                neuron.weights = neuron_weights\n",
        "                neuron.bias = neuron_bias"
      ],
      "metadata": {
        "id": "g3Jq30B-J1Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LossFunction:\n",
        "    \"\"\"A utility class for loss functions.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def cross_entropy(predicted_outputs: List[float], actual_outputs: List[float]) -> float:\n",
        "        \"\"\"Calculates the categorical cross-entropy loss.\n",
        "\n",
        "        Args:\n",
        "            predicted_outputs: A list of predicted probabilities for each class.\n",
        "            actual_outputs: A one-hot encoded list of actual output values.\n",
        "\n",
        "        Returns:\n",
        "            The categorical cross-entropy loss.\n",
        "        \"\"\"\n",
        "        # Clip the predicted values to prevent log(0)\n",
        "        predicted_outputs = np.clip(predicted_outputs, 1e-12, 1 - 1e-12)\n",
        "\n",
        "        # Calculate the cross-entropy loss\n",
        "        loss = -sum([actual_output * np.log(predicted_output)\n",
        "                     for predicted_output, actual_output in zip(predicted_outputs, actual_outputs)])\n",
        "        return loss / len(predicted_outputs)\n",
        "\n",
        "    @staticmethod\n",
        "    def mean_squared_error(predicted_outputs: List[float], actual_outputs: List[float]) -> float:\n",
        "        \"\"\"Calculates the mean squared error (MSE).\n",
        "\n",
        "        Args:\n",
        "            predicted_outputs: A list of predicted output values.\n",
        "            actual_outputs: A list of actual output values.\n",
        "\n",
        "        Returns:\n",
        "            The mean squared error.\n",
        "        \"\"\"\n",
        "        squared_errors = [(predicted_output - actual_output) ** 2\n",
        "                          for predicted_output, actual_output in zip(predicted_outputs, actual_outputs)]\n",
        "        return sum(squared_errors) / len(squared_errors)\n"
      ],
      "metadata": {
        "id": "anKsHgS8N-Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WeghitsInitializer:\n",
        "    \"\"\"A utility class for weight initialization.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def weights(num_inputs: int) -> List[float]:\n",
        "        \"\"\"Initializes weights with random values uniformly distributed between -0.1 and 0.1.\n",
        "\n",
        "        Args:\n",
        "            num_inputs: The number of inputs to the neuron.\n",
        "\n",
        "        Returns:\n",
        "            A list of initialized weights.\n",
        "        \"\"\"\n",
        "        return [random.uniform(-0.1, 0.1) for _ in range(num_inputs)]\n"
      ],
      "metadata": {
        "id": "da7SbIFhuipG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Workflow ðŸ”®: Load Dataset"
      ],
      "metadata": {
        "id": "ycREDLFRmswI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_loading()\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Iris Classification/Iris.csv'\n",
        "dataset, labels = load_dataset(file_path)\n",
        "test_split_data()\n",
        "training_data, validation_data, test_data = split_data(dataset)\n",
        "print(training_data)\n",
        "print(validation_data)\n",
        "print(test_data)"
      ],
      "metadata": {
        "id": "Rt4OIVV1msNB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1df8e90-2a9e-475c-baf0-3950e688a6d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded and validated successfully.\n",
            "Data splitting test passed successfully.\n",
            "[[38.0, 4.9, 3.1, 1.5, 0.1], [89.0, 5.6, 3.0, 4.1, 1.3], [141.0, 6.7, 3.1, 5.6, 2.4], [4.0, 4.6, 3.1, 1.5, 0.2], [99.0, 5.1, 2.5, 3.0, 1.1], [50.0, 5.0, 3.3, 1.4, 0.2], [111.0, 6.5, 3.2, 5.1, 2.0], [9.0, 4.4, 2.9, 1.4, 0.2], [91.0, 5.5, 2.6, 4.4, 1.2], [123.0, 7.7, 2.8, 6.7, 2.0], [109.0, 6.7, 2.5, 5.8, 1.8], [19.0, 5.7, 3.8, 1.7, 0.3], [24.0, 5.1, 3.3, 1.7, 0.5], [31.0, 4.8, 3.1, 1.6, 0.2], [86.0, 6.0, 3.4, 4.5, 1.6], [132.0, 7.9, 3.8, 6.4, 2.0], [112.0, 6.4, 2.7, 5.3, 1.9], [5.0, 5.0, 3.6, 1.4, 0.2], [15.0, 5.8, 4.0, 1.2, 0.2], [101.0, 6.3, 3.3, 6.0, 2.5], [80.0, 5.7, 2.6, 3.5, 1.0], [57.0, 6.3, 3.3, 4.7, 1.6], [118.0, 7.7, 3.8, 6.7, 2.2], [18.0, 5.1, 3.5, 1.4, 0.3], [126.0, 7.2, 3.2, 6.0, 1.8], [149.0, 6.2, 3.4, 5.4, 2.3], [150.0, 5.9, 3.0, 5.1, 1.8], [27.0, 5.0, 3.4, 1.6, 0.4], [66.0, 6.7, 3.1, 4.4, 1.4], [131.0, 7.4, 2.8, 6.1, 1.9], [114.0, 5.7, 2.5, 5.0, 2.0], [63.0, 6.0, 2.2, 4.0, 1.0], [55.0, 6.5, 2.8, 4.6, 1.5], [72.0, 6.1, 2.8, 4.0, 1.3], [3.0, 4.7, 3.2, 1.3, 0.2], [106.0, 7.6, 3.0, 6.6, 2.1], [125.0, 6.7, 3.3, 5.7, 2.1], [146.0, 6.7, 3.0, 5.2, 2.3], [36.0, 5.0, 3.2, 1.2, 0.2], [113.0, 6.8, 3.0, 5.5, 2.1], [6.0, 5.4, 3.9, 1.7, 0.4], [33.0, 5.2, 4.1, 1.5, 0.1], [51.0, 7.0, 3.2, 4.7, 1.4], [61.0, 5.0, 2.0, 3.5, 1.0], [107.0, 4.9, 2.5, 4.5, 1.7], [28.0, 5.2, 3.5, 1.5, 0.2], [93.0, 5.8, 2.6, 4.0, 1.2], [74.0, 6.1, 2.8, 4.7, 1.2], [65.0, 5.6, 2.9, 3.6, 1.3], [71.0, 5.9, 3.2, 4.8, 1.8], [96.0, 5.7, 3.0, 4.2, 1.2], [76.0, 6.6, 3.0, 4.4, 1.4], [29.0, 5.2, 3.4, 1.4, 0.2], [70.0, 5.6, 2.5, 3.9, 1.1], [7.0, 4.6, 3.4, 1.4, 0.3], [121.0, 6.9, 3.2, 5.7, 2.3], [17.0, 5.4, 3.9, 1.3, 0.4], [103.0, 7.1, 3.0, 5.9, 2.1], [145.0, 6.7, 3.3, 5.7, 2.5], [105.0, 6.5, 3.0, 5.8, 2.2], [81.0, 5.5, 2.4, 3.8, 1.1], [16.0, 5.7, 4.4, 1.5, 0.4], [62.0, 5.9, 3.0, 4.2, 1.5], [147.0, 6.3, 2.5, 5.0, 1.9], [10.0, 4.9, 3.1, 1.5, 0.1], [143.0, 5.8, 2.7, 5.1, 1.9], [69.0, 6.2, 2.2, 4.5, 1.5], [20.0, 5.1, 3.8, 1.5, 0.3], [95.0, 5.6, 2.7, 4.2, 1.3], [64.0, 6.1, 2.9, 4.7, 1.4], [148.0, 6.5, 3.0, 5.2, 2.0], [14.0, 4.3, 3.0, 1.1, 0.1], [82.0, 5.5, 2.4, 3.7, 1.0], [47.0, 5.1, 3.8, 1.6, 0.2], [94.0, 5.0, 2.3, 3.3, 1.0], [98.0, 6.2, 2.9, 4.3, 1.3], [41.0, 5.0, 3.5, 1.3, 0.3], [58.0, 4.9, 2.4, 3.3, 1.0], [142.0, 6.9, 3.1, 5.1, 2.3], [144.0, 6.8, 3.2, 5.9, 2.3], [120.0, 6.0, 2.2, 5.0, 1.5], [40.0, 5.1, 3.4, 1.5, 0.2], [90.0, 5.5, 2.5, 4.0, 1.3], [67.0, 5.6, 3.0, 4.5, 1.5], [85.0, 5.4, 3.0, 4.5, 1.5], [78.0, 6.7, 3.0, 5.0, 1.7], [1.0, 5.1, 3.5, 1.4, 0.2], [134.0, 6.3, 2.8, 5.1, 1.5], [39.0, 4.4, 3.0, 1.3, 0.2], [135.0, 6.1, 2.6, 5.6, 1.4], [79.0, 6.0, 2.9, 4.5, 1.5], [21.0, 5.4, 3.4, 1.7, 0.2], [52.0, 6.4, 3.2, 4.5, 1.5], [104.0, 6.3, 2.9, 5.6, 1.8], [25.0, 4.8, 3.4, 1.9, 0.2], [115.0, 5.8, 2.8, 5.1, 2.4], [124.0, 6.3, 2.7, 4.9, 1.8], [30.0, 4.7, 3.2, 1.6, 0.2], [73.0, 6.3, 2.5, 4.9, 1.5], [59.0, 6.6, 2.9, 4.6, 1.3], [34.0, 5.5, 4.2, 1.4, 0.2], [122.0, 5.6, 2.8, 4.9, 2.0], [49.0, 5.3, 3.7, 1.5, 0.2], [130.0, 7.2, 3.0, 5.8, 1.6], [53.0, 6.9, 3.1, 4.9, 1.5]]\n",
            "[[42.0, 4.5, 2.3, 1.3, 0.3], [102.0, 5.8, 2.7, 5.1, 1.9], [119.0, 7.7, 2.6, 6.9, 2.3], [32.0, 5.4, 3.4, 1.5, 0.4], [22.0, 5.1, 3.7, 1.5, 0.4], [60.0, 5.2, 2.7, 3.9, 1.4], [48.0, 4.6, 3.2, 1.4, 0.2], [87.0, 6.7, 3.1, 4.7, 1.5], [140.0, 6.9, 3.1, 5.4, 2.1], [92.0, 6.1, 3.0, 4.6, 1.4], [129.0, 6.4, 2.8, 5.6, 2.1], [12.0, 4.8, 3.4, 1.6, 0.2], [44.0, 5.0, 3.5, 1.6, 0.6], [97.0, 5.7, 2.9, 4.2, 1.3], [127.0, 6.2, 2.8, 4.8, 1.8], [45.0, 5.1, 3.8, 1.9, 0.4], [116.0, 6.4, 3.2, 5.3, 2.3], [128.0, 6.1, 3.0, 4.9, 1.8], [11.0, 5.4, 3.7, 1.5, 0.2], [137.0, 6.3, 3.4, 5.6, 2.4], [68.0, 5.8, 2.7, 4.1, 1.0], [88.0, 6.3, 2.3, 4.4, 1.3]]\n",
            "[[8.0, 5.0, 3.4, 1.5, 0.2], [46.0, 4.8, 3.0, 1.4, 0.3], [54.0, 5.5, 2.3, 4.0, 1.3], [56.0, 5.7, 2.8, 4.5, 1.3], [77.0, 6.8, 2.8, 4.8, 1.4], [138.0, 6.4, 3.1, 5.5, 1.8], [83.0, 5.8, 2.7, 3.9, 1.2], [117.0, 6.5, 3.0, 5.5, 1.8], [110.0, 7.2, 3.6, 6.1, 2.5], [43.0, 4.4, 3.2, 1.3, 0.2], [100.0, 5.7, 2.8, 4.1, 1.3], [84.0, 6.0, 2.7, 5.1, 1.6], [75.0, 6.4, 2.9, 4.3, 1.3], [35.0, 4.9, 3.1, 1.5, 0.1], [37.0, 5.5, 3.5, 1.3, 0.2], [26.0, 5.0, 3.0, 1.6, 0.2], [2.0, 4.9, 3.0, 1.4, 0.2], [23.0, 4.6, 3.6, 1.0, 0.2], [139.0, 6.0, 3.0, 4.8, 1.8], [108.0, 7.3, 2.9, 6.3, 1.8], [13.0, 4.8, 3.0, 1.4, 0.1], [133.0, 6.4, 2.8, 5.6, 2.2], [136.0, 7.7, 3.0, 6.1, 2.3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workflow ðŸ”®: Architecture"
      ],
      "metadata": {
        "id": "Iz1cl6tesGG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create input layer"
      ],
      "metadata": {
        "id": "r5fEpiz3swJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = WeghitsInitializer.weights(4)\n",
        "bias = random.uniform(-0.1, 0.1)\n",
        "input_neurons = [Neuron(weights=weights, bias=bias) for _ in range(4)]\n",
        "input_layer = Layer(neurons=input_neurons, activation= ActivationFunctions.relu, is_output_layer=False)\n",
        "\n",
        "# Print weights of input neurons\n",
        "for neuron in input_layer.neurons:\n",
        "    print(\"Input Neuron Weights:\", neuron.weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrEKj4tMsYN8",
        "outputId": "cea62d1c-e6a1-4ad4-82c4-87ee979e0916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Neuron Weights: [-0.012236743985385504, -0.07112458581385527, -0.02374207543011911, 0.09096790851691727]\n",
            "Input Neuron Weights: [-0.012236743985385504, -0.07112458581385527, -0.02374207543011911, 0.09096790851691727]\n",
            "Input Neuron Weights: [-0.012236743985385504, -0.07112458581385527, -0.02374207543011911, 0.09096790851691727]\n",
            "Input Neuron Weights: [-0.012236743985385504, -0.07112458581385527, -0.02374207543011911, 0.09096790851691727]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create hidden layer"
      ],
      "metadata": {
        "id": "lnY2OyY8wQI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = WeghitsInitializer.weights(4)\n",
        "bias = random.uniform(-0.1, 0.1)\n",
        "hidden_neurons = [Neuron(weights=weights, bias=bias) for _ in range(5)]\n",
        "hidden_layer = Layer(neurons=hidden_neurons, activation= ActivationFunctions.relu, is_output_layer=False)\n",
        "\n",
        "# Print weights of input neurons\n",
        "for neuron in hidden_layer.neurons:\n",
        "    print(\"Input Neuron Weights:\", neuron.weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bK5AUOBwUWP",
        "outputId": "840c8456-4145-42dd-88d8-99a792a3613b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Neuron Weights: [-0.0025985139119916456, 0.08967536608302626, 0.024991896165996758, -0.028760546744581533]\n",
            "Input Neuron Weights: [-0.0025985139119916456, 0.08967536608302626, 0.024991896165996758, -0.028760546744581533]\n",
            "Input Neuron Weights: [-0.0025985139119916456, 0.08967536608302626, 0.024991896165996758, -0.028760546744581533]\n",
            "Input Neuron Weights: [-0.0025985139119916456, 0.08967536608302626, 0.024991896165996758, -0.028760546744581533]\n",
            "Input Neuron Weights: [-0.0025985139119916456, 0.08967536608302626, 0.024991896165996758, -0.028760546744581533]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create output layer"
      ],
      "metadata": {
        "id": "UmGT2bpKw1in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = WeghitsInitializer.weights(5)\n",
        "bias = random.uniform(-0.1, 0.1)\n",
        "output_neurons = [Neuron(weights=weights, bias=bias) for _ in range(3)]\n",
        "output_layer = Layer(neurons=output_neurons, activation= ActivationFunctions.relu, is_output_layer=True)\n",
        "\n",
        "# Print weights of input neurons\n",
        "for neuron in output_layer.neurons:\n",
        "    print(\"Input Neuron Weights:\", neuron.weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKPIAROvw2kq",
        "outputId": "0640f32a-4d51-4ddd-e51f-69644884743a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Neuron Weights: [0.061455682311839954, -0.013101149463906475, 0.09484125191639023, 0.04079782617445696, -0.049328948326537274]\n",
            "Input Neuron Weights: [0.061455682311839954, -0.013101149463906475, 0.09484125191639023, 0.04079782617445696, -0.049328948326537274]\n",
            "Input Neuron Weights: [0.061455682311839954, -0.013101149463906475, 0.09484125191639023, 0.04079782617445696, -0.049328948326537274]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "n-c1fLTl18Ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define network architecture\n",
        "epochs = 100\n",
        "learning_rate = 0.01\n",
        "layers = [input_layer, hidden_layer, output_layer]\n",
        "\n",
        "# Initialize the network\n",
        "network = Network(layers=layers, epochs=epochs, learning_rate=learning_rate)\n",
        "\n",
        "num_samples = len(training_data)\n",
        "print(num_samples)\n",
        "print(training_data)\n",
        "print(labels)\n",
        "\n",
        "def match_and_combine(upper_list: List[List[float]], lower_list: List[List[float]]) -> List[Tuple[List[float], str]]:\n",
        "    # Create a dictionary from the lower list\n",
        "    lower_dict = {item[0]: item[1] for item in lower_list}\n",
        "\n",
        "    # Create a list to store the result tuples\n",
        "    result = []\n",
        "\n",
        "    # Iterate through the upper list\n",
        "    for item in upper_list:\n",
        "        key = item[0]\n",
        "        # Check if the key exists in the lower dictionary\n",
        "        if key in lower_dict:\n",
        "            # Create a tuple (without the key) and add to the result list\n",
        "            result.append((item[1:], lower_dict[key]))\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example usage\n",
        "combined = match_and_combine(training_data, labels)\n",
        "print(combined)\n",
        "\n",
        "network.train(combined)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "9Wl2HNYh19rX",
        "outputId": "c52a8d95-18af-4888-ff53-cc223d0f77a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105\n",
            "[[38.0, 4.9, 3.1, 1.5, 0.1], [89.0, 5.6, 3.0, 4.1, 1.3], [141.0, 6.7, 3.1, 5.6, 2.4], [4.0, 4.6, 3.1, 1.5, 0.2], [99.0, 5.1, 2.5, 3.0, 1.1], [50.0, 5.0, 3.3, 1.4, 0.2], [111.0, 6.5, 3.2, 5.1, 2.0], [9.0, 4.4, 2.9, 1.4, 0.2], [91.0, 5.5, 2.6, 4.4, 1.2], [123.0, 7.7, 2.8, 6.7, 2.0], [109.0, 6.7, 2.5, 5.8, 1.8], [19.0, 5.7, 3.8, 1.7, 0.3], [24.0, 5.1, 3.3, 1.7, 0.5], [31.0, 4.8, 3.1, 1.6, 0.2], [86.0, 6.0, 3.4, 4.5, 1.6], [132.0, 7.9, 3.8, 6.4, 2.0], [112.0, 6.4, 2.7, 5.3, 1.9], [5.0, 5.0, 3.6, 1.4, 0.2], [15.0, 5.8, 4.0, 1.2, 0.2], [101.0, 6.3, 3.3, 6.0, 2.5], [80.0, 5.7, 2.6, 3.5, 1.0], [57.0, 6.3, 3.3, 4.7, 1.6], [118.0, 7.7, 3.8, 6.7, 2.2], [18.0, 5.1, 3.5, 1.4, 0.3], [126.0, 7.2, 3.2, 6.0, 1.8], [149.0, 6.2, 3.4, 5.4, 2.3], [150.0, 5.9, 3.0, 5.1, 1.8], [27.0, 5.0, 3.4, 1.6, 0.4], [66.0, 6.7, 3.1, 4.4, 1.4], [131.0, 7.4, 2.8, 6.1, 1.9], [114.0, 5.7, 2.5, 5.0, 2.0], [63.0, 6.0, 2.2, 4.0, 1.0], [55.0, 6.5, 2.8, 4.6, 1.5], [72.0, 6.1, 2.8, 4.0, 1.3], [3.0, 4.7, 3.2, 1.3, 0.2], [106.0, 7.6, 3.0, 6.6, 2.1], [125.0, 6.7, 3.3, 5.7, 2.1], [146.0, 6.7, 3.0, 5.2, 2.3], [36.0, 5.0, 3.2, 1.2, 0.2], [113.0, 6.8, 3.0, 5.5, 2.1], [6.0, 5.4, 3.9, 1.7, 0.4], [33.0, 5.2, 4.1, 1.5, 0.1], [51.0, 7.0, 3.2, 4.7, 1.4], [61.0, 5.0, 2.0, 3.5, 1.0], [107.0, 4.9, 2.5, 4.5, 1.7], [28.0, 5.2, 3.5, 1.5, 0.2], [93.0, 5.8, 2.6, 4.0, 1.2], [74.0, 6.1, 2.8, 4.7, 1.2], [65.0, 5.6, 2.9, 3.6, 1.3], [71.0, 5.9, 3.2, 4.8, 1.8], [96.0, 5.7, 3.0, 4.2, 1.2], [76.0, 6.6, 3.0, 4.4, 1.4], [29.0, 5.2, 3.4, 1.4, 0.2], [70.0, 5.6, 2.5, 3.9, 1.1], [7.0, 4.6, 3.4, 1.4, 0.3], [121.0, 6.9, 3.2, 5.7, 2.3], [17.0, 5.4, 3.9, 1.3, 0.4], [103.0, 7.1, 3.0, 5.9, 2.1], [145.0, 6.7, 3.3, 5.7, 2.5], [105.0, 6.5, 3.0, 5.8, 2.2], [81.0, 5.5, 2.4, 3.8, 1.1], [16.0, 5.7, 4.4, 1.5, 0.4], [62.0, 5.9, 3.0, 4.2, 1.5], [147.0, 6.3, 2.5, 5.0, 1.9], [10.0, 4.9, 3.1, 1.5, 0.1], [143.0, 5.8, 2.7, 5.1, 1.9], [69.0, 6.2, 2.2, 4.5, 1.5], [20.0, 5.1, 3.8, 1.5, 0.3], [95.0, 5.6, 2.7, 4.2, 1.3], [64.0, 6.1, 2.9, 4.7, 1.4], [148.0, 6.5, 3.0, 5.2, 2.0], [14.0, 4.3, 3.0, 1.1, 0.1], [82.0, 5.5, 2.4, 3.7, 1.0], [47.0, 5.1, 3.8, 1.6, 0.2], [94.0, 5.0, 2.3, 3.3, 1.0], [98.0, 6.2, 2.9, 4.3, 1.3], [41.0, 5.0, 3.5, 1.3, 0.3], [58.0, 4.9, 2.4, 3.3, 1.0], [142.0, 6.9, 3.1, 5.1, 2.3], [144.0, 6.8, 3.2, 5.9, 2.3], [120.0, 6.0, 2.2, 5.0, 1.5], [40.0, 5.1, 3.4, 1.5, 0.2], [90.0, 5.5, 2.5, 4.0, 1.3], [67.0, 5.6, 3.0, 4.5, 1.5], [85.0, 5.4, 3.0, 4.5, 1.5], [78.0, 6.7, 3.0, 5.0, 1.7], [1.0, 5.1, 3.5, 1.4, 0.2], [134.0, 6.3, 2.8, 5.1, 1.5], [39.0, 4.4, 3.0, 1.3, 0.2], [135.0, 6.1, 2.6, 5.6, 1.4], [79.0, 6.0, 2.9, 4.5, 1.5], [21.0, 5.4, 3.4, 1.7, 0.2], [52.0, 6.4, 3.2, 4.5, 1.5], [104.0, 6.3, 2.9, 5.6, 1.8], [25.0, 4.8, 3.4, 1.9, 0.2], [115.0, 5.8, 2.8, 5.1, 2.4], [124.0, 6.3, 2.7, 4.9, 1.8], [30.0, 4.7, 3.2, 1.6, 0.2], [73.0, 6.3, 2.5, 4.9, 1.5], [59.0, 6.6, 2.9, 4.6, 1.3], [34.0, 5.5, 4.2, 1.4, 0.2], [122.0, 5.6, 2.8, 4.9, 2.0], [49.0, 5.3, 3.7, 1.5, 0.2], [130.0, 7.2, 3.0, 5.8, 1.6], [53.0, 6.9, 3.1, 4.9, 1.5]]\n",
            "[[1.0, 'Iris-setosa'], [2.0, 'Iris-setosa'], [3.0, 'Iris-setosa'], [4.0, 'Iris-setosa'], [5.0, 'Iris-setosa'], [6.0, 'Iris-setosa'], [7.0, 'Iris-setosa'], [8.0, 'Iris-setosa'], [9.0, 'Iris-setosa'], [10.0, 'Iris-setosa'], [11.0, 'Iris-setosa'], [12.0, 'Iris-setosa'], [13.0, 'Iris-setosa'], [14.0, 'Iris-setosa'], [15.0, 'Iris-setosa'], [16.0, 'Iris-setosa'], [17.0, 'Iris-setosa'], [18.0, 'Iris-setosa'], [19.0, 'Iris-setosa'], [20.0, 'Iris-setosa'], [21.0, 'Iris-setosa'], [22.0, 'Iris-setosa'], [23.0, 'Iris-setosa'], [24.0, 'Iris-setosa'], [25.0, 'Iris-setosa'], [26.0, 'Iris-setosa'], [27.0, 'Iris-setosa'], [28.0, 'Iris-setosa'], [29.0, 'Iris-setosa'], [30.0, 'Iris-setosa'], [31.0, 'Iris-setosa'], [32.0, 'Iris-setosa'], [33.0, 'Iris-setosa'], [34.0, 'Iris-setosa'], [35.0, 'Iris-setosa'], [36.0, 'Iris-setosa'], [37.0, 'Iris-setosa'], [38.0, 'Iris-setosa'], [39.0, 'Iris-setosa'], [40.0, 'Iris-setosa'], [41.0, 'Iris-setosa'], [42.0, 'Iris-setosa'], [43.0, 'Iris-setosa'], [44.0, 'Iris-setosa'], [45.0, 'Iris-setosa'], [46.0, 'Iris-setosa'], [47.0, 'Iris-setosa'], [48.0, 'Iris-setosa'], [49.0, 'Iris-setosa'], [50.0, 'Iris-setosa'], [51.0, 'Iris-versicolor'], [52.0, 'Iris-versicolor'], [53.0, 'Iris-versicolor'], [54.0, 'Iris-versicolor'], [55.0, 'Iris-versicolor'], [56.0, 'Iris-versicolor'], [57.0, 'Iris-versicolor'], [58.0, 'Iris-versicolor'], [59.0, 'Iris-versicolor'], [60.0, 'Iris-versicolor'], [61.0, 'Iris-versicolor'], [62.0, 'Iris-versicolor'], [63.0, 'Iris-versicolor'], [64.0, 'Iris-versicolor'], [65.0, 'Iris-versicolor'], [66.0, 'Iris-versicolor'], [67.0, 'Iris-versicolor'], [68.0, 'Iris-versicolor'], [69.0, 'Iris-versicolor'], [70.0, 'Iris-versicolor'], [71.0, 'Iris-versicolor'], [72.0, 'Iris-versicolor'], [73.0, 'Iris-versicolor'], [74.0, 'Iris-versicolor'], [75.0, 'Iris-versicolor'], [76.0, 'Iris-versicolor'], [77.0, 'Iris-versicolor'], [78.0, 'Iris-versicolor'], [79.0, 'Iris-versicolor'], [80.0, 'Iris-versicolor'], [81.0, 'Iris-versicolor'], [82.0, 'Iris-versicolor'], [83.0, 'Iris-versicolor'], [84.0, 'Iris-versicolor'], [85.0, 'Iris-versicolor'], [86.0, 'Iris-versicolor'], [87.0, 'Iris-versicolor'], [88.0, 'Iris-versicolor'], [89.0, 'Iris-versicolor'], [90.0, 'Iris-versicolor'], [91.0, 'Iris-versicolor'], [92.0, 'Iris-versicolor'], [93.0, 'Iris-versicolor'], [94.0, 'Iris-versicolor'], [95.0, 'Iris-versicolor'], [96.0, 'Iris-versicolor'], [97.0, 'Iris-versicolor'], [98.0, 'Iris-versicolor'], [99.0, 'Iris-versicolor'], [100.0, 'Iris-versicolor'], [101.0, 'Iris-virginica'], [102.0, 'Iris-virginica'], [103.0, 'Iris-virginica'], [104.0, 'Iris-virginica'], [105.0, 'Iris-virginica'], [106.0, 'Iris-virginica'], [107.0, 'Iris-virginica'], [108.0, 'Iris-virginica'], [109.0, 'Iris-virginica'], [110.0, 'Iris-virginica'], [111.0, 'Iris-virginica'], [112.0, 'Iris-virginica'], [113.0, 'Iris-virginica'], [114.0, 'Iris-virginica'], [115.0, 'Iris-virginica'], [116.0, 'Iris-virginica'], [117.0, 'Iris-virginica'], [118.0, 'Iris-virginica'], [119.0, 'Iris-virginica'], [120.0, 'Iris-virginica'], [121.0, 'Iris-virginica'], [122.0, 'Iris-virginica'], [123.0, 'Iris-virginica'], [124.0, 'Iris-virginica'], [125.0, 'Iris-virginica'], [126.0, 'Iris-virginica'], [127.0, 'Iris-virginica'], [128.0, 'Iris-virginica'], [129.0, 'Iris-virginica'], [130.0, 'Iris-virginica'], [131.0, 'Iris-virginica'], [132.0, 'Iris-virginica'], [133.0, 'Iris-virginica'], [134.0, 'Iris-virginica'], [135.0, 'Iris-virginica'], [136.0, 'Iris-virginica'], [137.0, 'Iris-virginica'], [138.0, 'Iris-virginica'], [139.0, 'Iris-virginica'], [140.0, 'Iris-virginica'], [141.0, 'Iris-virginica'], [142.0, 'Iris-virginica'], [143.0, 'Iris-virginica'], [144.0, 'Iris-virginica'], [145.0, 'Iris-virginica'], [146.0, 'Iris-virginica'], [147.0, 'Iris-virginica'], [148.0, 'Iris-virginica'], [149.0, 'Iris-virginica'], [150.0, 'Iris-virginica']]\n",
            "[([4.9, 3.1, 1.5, 0.1], 'Iris-setosa'), ([5.6, 3.0, 4.1, 1.3], 'Iris-versicolor'), ([6.7, 3.1, 5.6, 2.4], 'Iris-virginica'), ([4.6, 3.1, 1.5, 0.2], 'Iris-setosa'), ([5.1, 2.5, 3.0, 1.1], 'Iris-versicolor'), ([5.0, 3.3, 1.4, 0.2], 'Iris-setosa'), ([6.5, 3.2, 5.1, 2.0], 'Iris-virginica'), ([4.4, 2.9, 1.4, 0.2], 'Iris-setosa'), ([5.5, 2.6, 4.4, 1.2], 'Iris-versicolor'), ([7.7, 2.8, 6.7, 2.0], 'Iris-virginica'), ([6.7, 2.5, 5.8, 1.8], 'Iris-virginica'), ([5.7, 3.8, 1.7, 0.3], 'Iris-setosa'), ([5.1, 3.3, 1.7, 0.5], 'Iris-setosa'), ([4.8, 3.1, 1.6, 0.2], 'Iris-setosa'), ([6.0, 3.4, 4.5, 1.6], 'Iris-versicolor'), ([7.9, 3.8, 6.4, 2.0], 'Iris-virginica'), ([6.4, 2.7, 5.3, 1.9], 'Iris-virginica'), ([5.0, 3.6, 1.4, 0.2], 'Iris-setosa'), ([5.8, 4.0, 1.2, 0.2], 'Iris-setosa'), ([6.3, 3.3, 6.0, 2.5], 'Iris-virginica'), ([5.7, 2.6, 3.5, 1.0], 'Iris-versicolor'), ([6.3, 3.3, 4.7, 1.6], 'Iris-versicolor'), ([7.7, 3.8, 6.7, 2.2], 'Iris-virginica'), ([5.1, 3.5, 1.4, 0.3], 'Iris-setosa'), ([7.2, 3.2, 6.0, 1.8], 'Iris-virginica'), ([6.2, 3.4, 5.4, 2.3], 'Iris-virginica'), ([5.9, 3.0, 5.1, 1.8], 'Iris-virginica'), ([5.0, 3.4, 1.6, 0.4], 'Iris-setosa'), ([6.7, 3.1, 4.4, 1.4], 'Iris-versicolor'), ([7.4, 2.8, 6.1, 1.9], 'Iris-virginica'), ([5.7, 2.5, 5.0, 2.0], 'Iris-virginica'), ([6.0, 2.2, 4.0, 1.0], 'Iris-versicolor'), ([6.5, 2.8, 4.6, 1.5], 'Iris-versicolor'), ([6.1, 2.8, 4.0, 1.3], 'Iris-versicolor'), ([4.7, 3.2, 1.3, 0.2], 'Iris-setosa'), ([7.6, 3.0, 6.6, 2.1], 'Iris-virginica'), ([6.7, 3.3, 5.7, 2.1], 'Iris-virginica'), ([6.7, 3.0, 5.2, 2.3], 'Iris-virginica'), ([5.0, 3.2, 1.2, 0.2], 'Iris-setosa'), ([6.8, 3.0, 5.5, 2.1], 'Iris-virginica'), ([5.4, 3.9, 1.7, 0.4], 'Iris-setosa'), ([5.2, 4.1, 1.5, 0.1], 'Iris-setosa'), ([7.0, 3.2, 4.7, 1.4], 'Iris-versicolor'), ([5.0, 2.0, 3.5, 1.0], 'Iris-versicolor'), ([4.9, 2.5, 4.5, 1.7], 'Iris-virginica'), ([5.2, 3.5, 1.5, 0.2], 'Iris-setosa'), ([5.8, 2.6, 4.0, 1.2], 'Iris-versicolor'), ([6.1, 2.8, 4.7, 1.2], 'Iris-versicolor'), ([5.6, 2.9, 3.6, 1.3], 'Iris-versicolor'), ([5.9, 3.2, 4.8, 1.8], 'Iris-versicolor'), ([5.7, 3.0, 4.2, 1.2], 'Iris-versicolor'), ([6.6, 3.0, 4.4, 1.4], 'Iris-versicolor'), ([5.2, 3.4, 1.4, 0.2], 'Iris-setosa'), ([5.6, 2.5, 3.9, 1.1], 'Iris-versicolor'), ([4.6, 3.4, 1.4, 0.3], 'Iris-setosa'), ([6.9, 3.2, 5.7, 2.3], 'Iris-virginica'), ([5.4, 3.9, 1.3, 0.4], 'Iris-setosa'), ([7.1, 3.0, 5.9, 2.1], 'Iris-virginica'), ([6.7, 3.3, 5.7, 2.5], 'Iris-virginica'), ([6.5, 3.0, 5.8, 2.2], 'Iris-virginica'), ([5.5, 2.4, 3.8, 1.1], 'Iris-versicolor'), ([5.7, 4.4, 1.5, 0.4], 'Iris-setosa'), ([5.9, 3.0, 4.2, 1.5], 'Iris-versicolor'), ([6.3, 2.5, 5.0, 1.9], 'Iris-virginica'), ([4.9, 3.1, 1.5, 0.1], 'Iris-setosa'), ([5.8, 2.7, 5.1, 1.9], 'Iris-virginica'), ([6.2, 2.2, 4.5, 1.5], 'Iris-versicolor'), ([5.1, 3.8, 1.5, 0.3], 'Iris-setosa'), ([5.6, 2.7, 4.2, 1.3], 'Iris-versicolor'), ([6.1, 2.9, 4.7, 1.4], 'Iris-versicolor'), ([6.5, 3.0, 5.2, 2.0], 'Iris-virginica'), ([4.3, 3.0, 1.1, 0.1], 'Iris-setosa'), ([5.5, 2.4, 3.7, 1.0], 'Iris-versicolor'), ([5.1, 3.8, 1.6, 0.2], 'Iris-setosa'), ([5.0, 2.3, 3.3, 1.0], 'Iris-versicolor'), ([6.2, 2.9, 4.3, 1.3], 'Iris-versicolor'), ([5.0, 3.5, 1.3, 0.3], 'Iris-setosa'), ([4.9, 2.4, 3.3, 1.0], 'Iris-versicolor'), ([6.9, 3.1, 5.1, 2.3], 'Iris-virginica'), ([6.8, 3.2, 5.9, 2.3], 'Iris-virginica'), ([6.0, 2.2, 5.0, 1.5], 'Iris-virginica'), ([5.1, 3.4, 1.5, 0.2], 'Iris-setosa'), ([5.5, 2.5, 4.0, 1.3], 'Iris-versicolor'), ([5.6, 3.0, 4.5, 1.5], 'Iris-versicolor'), ([5.4, 3.0, 4.5, 1.5], 'Iris-versicolor'), ([6.7, 3.0, 5.0, 1.7], 'Iris-versicolor'), ([5.1, 3.5, 1.4, 0.2], 'Iris-setosa'), ([6.3, 2.8, 5.1, 1.5], 'Iris-virginica'), ([4.4, 3.0, 1.3, 0.2], 'Iris-setosa'), ([6.1, 2.6, 5.6, 1.4], 'Iris-virginica'), ([6.0, 2.9, 4.5, 1.5], 'Iris-versicolor'), ([5.4, 3.4, 1.7, 0.2], 'Iris-setosa'), ([6.4, 3.2, 4.5, 1.5], 'Iris-versicolor'), ([6.3, 2.9, 5.6, 1.8], 'Iris-virginica'), ([4.8, 3.4, 1.9, 0.2], 'Iris-setosa'), ([5.8, 2.8, 5.1, 2.4], 'Iris-virginica'), ([6.3, 2.7, 4.9, 1.8], 'Iris-virginica'), ([4.7, 3.2, 1.6, 0.2], 'Iris-setosa'), ([6.3, 2.5, 4.9, 1.5], 'Iris-versicolor'), ([6.6, 2.9, 4.6, 1.3], 'Iris-versicolor'), ([5.5, 4.2, 1.4, 0.2], 'Iris-setosa'), ([5.6, 2.8, 4.9, 2.0], 'Iris-virginica'), ([5.3, 3.7, 1.5, 0.2], 'Iris-setosa'), ([7.2, 3.0, 5.8, 1.6], 'Iris-virginica'), ([6.9, 3.1, 4.9, 1.5], 'Iris-versicolor')]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "can't multiply sequence by non-int of type 'numpy.float64'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-46d63050e404>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-15881fbbd735>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-15881fbbd735>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, predicted, actual)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;34m\"\"\"Calculates the loss for the predictions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mLossFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-bb076c1d84d9>\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(predicted_outputs, actual_outputs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Calculate the cross-entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         loss = -sum([actual_output * np.log(predicted_output)\n\u001b[0m\u001b[1;32m     20\u001b[0m                      for predicted_output, actual_output in zip(predicted_outputs, actual_outputs)])\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-bb076c1d84d9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Calculate the cross-entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         loss = -sum([actual_output * np.log(predicted_output)\n\u001b[0m\u001b[1;32m     20\u001b[0m                      for predicted_output, actual_output in zip(predicted_outputs, actual_outputs)])\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'numpy.float64'"
          ]
        }
      ]
    }
  ]
}