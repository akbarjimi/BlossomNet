{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tWngIP5CvyU7H90KXCeYbnI3H7fM6e3c",
      "authorship_tag": "ABX9TyPw0muZFAELBiUMSfq7iKlq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load Dataset 🔄"
      ],
      "metadata": {
        "id": "lF8gLedNdQv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import csv\n",
        "from typing import List, Tuple, Any\n",
        "import math\n",
        "import pickle"
      ],
      "metadata": {
        "id": "y5hCJB4_FeVh"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "d-y8bz9OCL-A"
      },
      "outputs": [],
      "source": [
        "def load_dataset(file_path: str) -> Tuple[List[List[float]], List[Any]]:\n",
        "    \"\"\"\n",
        "    Loads the dataset from a CSV file.\n",
        "\n",
        "    This function assumes that the CSV file has a header row and that:\n",
        "    - All columns except the last one are features (converted to float).\n",
        "    - The last column is the label (left as a string; convert if needed).\n",
        "\n",
        "    Note:\n",
        "      The current implementation is particularly suited for datasets like the Iris dataset.\n",
        "      For other datasets, you might want to modify the logic (e.g., to change the label column index).\n",
        "\n",
        "    Parameters:\n",
        "      file_path (str): Path to the dataset file.\n",
        "\n",
        "    Returns:\n",
        "      Tuple[List[List[float]], List[Any]]:\n",
        "          - data: A list of rows, each row is a list of features as floats.\n",
        "          - labels: A list of labels corresponding to each row.\n",
        "    \"\"\"\n",
        "    data: List[List[float]] = []\n",
        "    labels: List[Any] = []\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            reader = csv.reader(file)\n",
        "            header = next(reader, None)  # Skip header row if exists\n",
        "\n",
        "            # Process each row in the CSV file\n",
        "            for row in reader:\n",
        "                if row:  # Ensure the row is not empty\n",
        "                    # Convert all columns except the last one into floats (features)\n",
        "                    try:\n",
        "                        features = [float(item) for item in row[:-1]]\n",
        "                    except ValueError as ve:\n",
        "                        print(f\"Could not convert features to float in row: {row}. Error: {ve}\")\n",
        "                        continue\n",
        "                    # The last column is considered the label (remains as string or processed further)\n",
        "                    label = row[-1]\n",
        "\n",
        "                    data.append(features)\n",
        "                    labels.append(label)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the file: {e}\")\n",
        "\n",
        "    return data, labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    dataset, labels = load_dataset('/content/drive/MyDrive/Colab Notebooks/Iris Classification/Iris.csv')\n",
        "    print(dataset[:3],\"\\n\")\n",
        "    print(labels[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5moBn_B-ewV",
        "outputId": "5b5a352a-aaad-4922-aba2-422c17fd8036"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0, 5.1, 3.5, 1.4, 0.2], [2.0, 4.9, 3.0, 1.4, 0.2], [3.0, 4.7, 3.2, 1.3, 0.2]] \n",
            "\n",
            "['Iris-setosa', 'Iris-setosa', 'Iris-setosa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_dataset(dataset: List[List[Any]]) -> bool:\n",
        "    \"\"\"\n",
        "    بررسی صحت ساختار و یکپارچگی مجموعه داده.\n",
        "\n",
        "    این تابع بررسی می‌کند که:\n",
        "      - مجموعه داده خالی نباشد.\n",
        "      - تمامی ردیف‌های مجموعه داده دارای تعداد ویژگی یکسان باشند.\n",
        "\n",
        "    پارامترها:\n",
        "      dataset (List[List[Any]]): لیستی از ردیف‌های داده؛ هر ردیف نیز لیستی از ویژگی‌هاست.\n",
        "\n",
        "    خروجی:\n",
        "      bool: اگر مجموعه داده صحیح باشد، مقدار True و در غیر این صورت False برمی‌گرداند.\n",
        "    \"\"\"\n",
        "    # اگر مجموعه داده خالی باشد، معتبر نیست.\n",
        "    if not dataset:\n",
        "        return False\n",
        "\n",
        "    # تعداد ویژگی‌ها در اولین ردیف را به عنوان مبنا در نظر می‌گیریم.\n",
        "    num_features = len(dataset[0])\n",
        "    for row in dataset:\n",
        "        # بررسی می‌کند که هر ردیف همان تعداد ویژگی با ردیف اولیه داشته باشد.\n",
        "        if len(row) != num_features:\n",
        "            return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "br0ElRjWdxiw"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # نمونه‌ای از یک مجموعه داده معتبر (مثلاً برای دیتاست Iris یا سایر دیتاست‌ها)\n",
        "    valid_dataset = [\n",
        "        [5.1, 3.5, 1.4, 0.2],\n",
        "        [4.9, 3.0, 1.4, 0.2],\n",
        "        [6.2, 3.4, 5.4, 2.3]\n",
        "    ]\n",
        "\n",
        "    # نمونه‌ای از یک مجموعه داده نامعتبر (یک ردیف با تعداد ویژگی متفاوت)\n",
        "    invalid_dataset = [\n",
        "        [5.1, 3.5, 1.4, 0.2],\n",
        "        [4.9, 3.0, 1.4],  # این ردیف تعداد ویژگی کمتری نسبت به بقیه دارد.\n",
        "        [6.2, 3.4, 5.4, 2.3]\n",
        "    ]\n",
        "\n",
        "    # اجرای تابع و چاپ نتایج\n",
        "    print(\"مجموعه داده معتبر:\", validate_dataset(valid_dataset))    # خروجی: True\n",
        "    print(\"مجموعه داده نامعتبر:\", validate_dataset(invalid_dataset))  # خروجی: False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuwqFF-WEiB9",
        "outputId": "248702ae-e8ed-4a7e-84e9-81713273cd25"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "مجموعه داده معتبر: True\n",
            "مجموعه داده نامعتبر: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_data_loading(file_path: str) -> None:\n",
        "    \"\"\"\n",
        "    آزمون عملکرد تابع بارگذاری داده.\n",
        "\n",
        "    ورودی:\n",
        "      file_path (str): مسیر فایل CSV برای بارگذاری داده‌ها.\n",
        "\n",
        "    خروجی:\n",
        "      None: نتیجه تست از طریق چاپ پیام به کنسول نمایش داده می‌شود.\n",
        "\n",
        "    توضیحات:\n",
        "      این تابع ابتدا داده‌ها و برچسب‌ها را از فایل ورودی با استفاده از load_dataset می‌خواند.\n",
        "      سپس با استفاده از validate_dataset اعتبار داده‌ها را بررسی می‌کند.\n",
        "      در صورت موفقیت، پیام موفقیت را چاپ می‌کند و در صورت بروز خطا، پیغام مناسب را چاپ می‌کند.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        dataset, labels = load_dataset(file_path)\n",
        "        assert validate_dataset(dataset), \"Dataset validation failed.\"\n",
        "        print(\"Data loaded and validated successfully.\")\n",
        "    except AssertionError as error:\n",
        "        print(f\"Test failed: {error}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"File not found. Make sure the dataset file exists at the specified path.\")\n",
        "    except Exception as error:\n",
        "        print(f\"An error occurred: {error}\")"
      ],
      "metadata": {
        "id": "mKGpyOkmd8ke"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_loading('/content/drive/MyDrive/Colab Notebooks/Iris Classification/Iris.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "085zceueHOJl",
        "outputId": "28327a88-8b43-4f80-901e-c478870e1d8a"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded and validated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Normalize the Data 🔄"
      ],
      "metadata": {
        "id": "PBavN6UwmFU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transpose(data: List[List]) -> List[List]:\n",
        "    \"\"\"\n",
        "    ترانهاده کردن یک لیست دوبعدی.\n",
        "\n",
        "    پارامترها:\n",
        "        data (List[List]): داده‌ها به‌صورت لیست از لیست‌ها (مثلاً ماتریس)\n",
        "\n",
        "    خروجی:\n",
        "        List[List]: داده‌های ترانهاده‌شده\n",
        "    \"\"\"\n",
        "    # علامت ستاره داده ها را آنپک میکند\n",
        "    # تابع زیپ ستون n از هر سطر را کنار هم قرار میدهد و بدین وسیله داده ها را ترانسپوزه میکند.\n",
        "    return [list(row) for row in zip(*data)]"
      ],
      "metadata": {
        "id": "ILbm5BhKp2fJ"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def min_max_normalizer(data: List[List[float]]) -> List[List[float]]:\n",
        "    \"\"\"\n",
        "    این تابع داده‌های عددی رو با استفاده از روش Min-Max نرمال می‌کنه.\n",
        "    یعنی همه‌ی اعداد رو بین 0 و 1 میاره، تا مقایسه و آموزش مدل ساده‌تر بشه.\n",
        "    \"\"\"\n",
        "\n",
        "    # چرخوندن ماتریس برای بررسی هر ویژگی به‌صورت ستونی\n",
        "    transposed_data = transpose(data)\n",
        "\n",
        "    # پیدا کردن مینیمم و ماکزیمم هر ستون\n",
        "    min_vals = [min(col) for col in transposed_data]\n",
        "    max_vals = [max(col) for col in transposed_data]\n",
        "\n",
        "    scaled_data = []\n",
        "\n",
        "    # نرمال کردن هر مقدار با استفاده از فرمول min-max\n",
        "    for row_index, row in enumerate(data):\n",
        "        scaled_row = []\n",
        "        for val, min_val, max_val in zip(row, min_vals, max_vals):\n",
        "            range_val = max_val - min_val\n",
        "            if range_val == 0:\n",
        "                scaled_row.append(0.0)  # اگر همه مقادیر برابر بودن، خروجی رو صفر بزار\n",
        "            else:\n",
        "                scaled_val = (val - min_val) / range_val\n",
        "                scaled_row.append(scaled_val)\n",
        "        scaled_data.append(scaled_row)\n",
        "\n",
        "    return scaled_data"
      ],
      "metadata": {
        "id": "GHEPq3Qnm84C"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(dataset: List[List[float]], training_size: float = 0.7, validation_size: float = 0.0) -> Tuple[List[List[float]], List[List[float]], List[List[float]]]:\n",
        "    \"\"\"\n",
        "    این تابع دیتاست رو به سه بخش تقسیم می‌کنه: آموزش، اعتبارسنجی و تست.\n",
        "\n",
        "    پارامترها:\n",
        "    dataset (list of lists): دیتای ورودی که می‌خوایم تقسیمش کنیم.\n",
        "    training_size (float): درصدی از دیتا که برای آموزش استفاده میشه (مثلاً ۰.۷ یعنی ۷۰٪).\n",
        "    validation_size (float): درصدی از دیتا که برای اعتبارسنجی استفاده میشه (مثلاً ۰.۱۵ یعنی ۱۵٪).\n",
        "\n",
        "    خروجی:\n",
        "    یه تاپل شامل سه تا لیسته: دیتای آموزش، دیتای اعتبارسنجی، دیتای تست.\n",
        "    \"\"\"\n",
        "\n",
        "    # اول دیتا رو به صورت تصادفی قاطی می‌کنیم که ترتیبش تأثیر نذاره\n",
        "    random.shuffle(dataset)\n",
        "\n",
        "    # اندازه کل دیتا\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # محاسبه مرز بین بخش‌ها\n",
        "    train_end = int(training_size * total_size)\n",
        "    val_end = int((training_size) * total_size)\n",
        "\n",
        "    # برش دادن دیتا بر اساس درصدها\n",
        "    training_data = dataset[:train_end]\n",
        "    # validation_data = dataset[train_end:val_end]\n",
        "    validation_data = [[]]\n",
        "    test_data = dataset[val_end:]\n",
        "\n",
        "    return training_data, validation_data, test_data\n"
      ],
      "metadata": {
        "id": "JUk8uAB0k1F1"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    [5.1, 3.5, 1.4, 0.2],\n",
        "    [4.9, 3.0, 1.4, 0.2],\n",
        "    [6.2, 3.4, 5.4, 2.3],\n",
        "    [5.9, 3.0, 5.1, 1.8],\n",
        "    [5.4, 3.9, 1.7, 0.4],\n",
        "    [6.7, 3.1, 4.7, 1.5],\n",
        "    [5.6, 2.8, 4.9, 2.0],\n",
        "    [5.7, 2.1, 4.0, 2.1],\n",
        "    [5.8, 2.2, 9.4, 1.2],\n",
        "    [5.9, 2.3, 5.5, 0.8],\n",
        "]\n",
        "\n",
        "train, val, test = split_data(data, training_size=0.5, validation_size=0.2)\n",
        "print(\"Train:\", train)\n",
        "print(\"Validation:\", val)\n",
        "print(\"Test:\", test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vsrx7eIk6nIH",
        "outputId": "98e312bb-03ad-4fd3-dc42-c3e4622e579d"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: [[6.2, 3.4, 5.4, 2.3], [5.9, 2.3, 5.5, 0.8], [5.6, 2.8, 4.9, 2.0], [5.9, 3.0, 5.1, 1.8], [5.4, 3.9, 1.7, 0.4]]\n",
            "Validation: [[5.7, 2.1, 4.0, 2.1], [5.1, 3.5, 1.4, 0.2]]\n",
            "Test: [[6.7, 3.1, 4.7, 1.5], [5.8, 2.2, 9.4, 1.2], [4.9, 3.0, 1.4, 0.2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_split_data():\n",
        "    \"\"\"\n",
        "    این تابع بررسی می‌کنه که تابع split_data درست کار می‌کنه یا نه.\n",
        "    توی تست، اول دیتاست رو لود می‌کنیم، بعد نرمالایزش می‌کنیم، بعد تقسیمش می‌کنیم.\n",
        "    بعدش چک می‌کنیم که هر بخش از دیتا خالی نباشه و مجموعشون هم برابر با دیتای اولیه باشه.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # مسیر فایل دیتاست\n",
        "        file_path = '/content/drive/MyDrive/Colab Notebooks/Iris Classification/Iris.csv'\n",
        "\n",
        "        # لود کردن دیتا و لیبل‌ها\n",
        "        dataset, labels = load_dataset(file_path)\n",
        "\n",
        "        # نرمال‌سازی دیتا با Min-Max\n",
        "        dataset = min_max_normalizer(dataset)\n",
        "\n",
        "        # تقسیم دیتا به سه بخش\n",
        "        training_data, validation_data, test_data = split_data(dataset)\n",
        "\n",
        "        # تست اینکه هیچ‌کدوم از بخش‌ها خالی نباشه\n",
        "        assert len(training_data) > 0, \"داده‌های آموزش خالیه.\"\n",
        "        assert len(validation_data) > 0, \"داده‌های اعتبارسنجی خالیه.\"\n",
        "        assert len(test_data) > 0, \"داده‌های تست خالیه.\"\n",
        "\n",
        "        # تست اینکه تعداد کل داده‌ها تغییر نکرده باشه\n",
        "        total_size = len(training_data) + len(validation_data) + len(test_data)\n",
        "        assert total_size == len(dataset), \"خطا در تقسیم دیتا: تعداد نهایی با اولیه نمی‌خونه.\"\n",
        "\n",
        "        print(\"✅ تست تقسیم دیتا با موفقیت انجام شد.\")\n",
        "\n",
        "    except AssertionError as error:\n",
        "        print(f\"❌ تست شکست خورد: {error}\")\n",
        "\n",
        "    except Exception as error:\n",
        "        print(f\"⚠️ یه خطایی پیش اومد: {error}\")\n"
      ],
      "metadata": {
        "id": "EH-SJwTEk4PT"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_split_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH4BxNlN7rjY",
        "outputId": "e57979c5-290d-4e94-a297-12ca482197ec"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ تست تقسیم دیتا با موفقیت انجام شد.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Define the Architecture 🏗"
      ],
      "metadata": {
        "id": "VUV-OoGf9CoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron:\n",
        "    \"\"\"کلاس یک نورون ساده که برای محاسبه خروجی، گرادیان و به‌روزرسانی وزن‌ها استفاده می‌شود.\"\"\"\n",
        "\n",
        "    def __init__(self, weights: List[float], bias: float = None):\n",
        "        \"\"\"\n",
        "        نورون با وزن‌ها و بایاس داده‌شده مقداردهی اولیه می‌شود.\n",
        "\n",
        "        آرگومان‌ها:\n",
        "        weights: لیستی از وزن‌ها برای ورودی‌های نورون.\n",
        "        bias: مقدار بایاس برای نورون. اگر داده نشود، یک مقدار تصادفی بین -0.1 و 0.1 به آن اختصاص می‌یابد.\n",
        "        \"\"\"\n",
        "        self.weights = weights\n",
        "        self.bias = bias if bias is not None else random.uniform(-0.1, 0.1)\n",
        "        self.inputs = []  # ورودی‌های نورون را ذخیره می‌کند\n",
        "        self.output = 0   # خروجی نورون را ذخیره می‌کند\n",
        "\n",
        "    def forward(self, inputs: List[float]) -> float:\n",
        "        \"\"\"\n",
        "        خروجی نورون را با استفاده از ورودی‌ها و وزن‌ها محاسبه می‌کند.\n",
        "\n",
        "        آرگومان‌ها:\n",
        "        inputs: لیستی از ورودی‌ها به نورون.\n",
        "\n",
        "        برمی‌گرداند:\n",
        "        خروجی نورون.\n",
        "        \"\"\"\n",
        "        self.inputs = inputs  # ذخیره ورودی‌ها برای استفاده در مراحل بعدی\n",
        "        weighted_sum = sum([input_ * weight for input_, weight in zip(inputs, self.weights)])\n",
        "        self.output = weighted_sum + self.bias  # جمع کردن وزن‌ها و بایاس\n",
        "        return self.output\n",
        "\n",
        "    # def activation(self, output: float) -> float:\n",
        "    #     \"\"\"\n",
        "    #     تابع فعال‌سازی را بر روی خروجی نورون اعمال می‌کند.\n",
        "\n",
        "    #     در اینجا، به طور ساده از تابع سیگموید استفاده می‌کنیم.\n",
        "    #     \"\"\"\n",
        "\n",
        "    #     # از تابع فعال‌سازی سیگموید استفاده می‌کنیم\n",
        "    #     return 1 / (1 + (2.718 ** -output))  # این یعنی سیگموید\n",
        "\n",
        "    def activation(self, output: float) -> float:\n",
        "        return max(0, output)\n",
        "\n",
        "    def compute_gradient(self, delta: float) -> List[float]:\n",
        "        \"\"\"\n",
        "        گرادیان برای وزن‌ها را با استفاده از دلتا (سیگنال خطای لایه بعدی) محاسبه می‌کند.\n",
        "\n",
        "        آرگومان‌ها:\n",
        "        delta: سیگنال خطا از لایه بعدی.\n",
        "\n",
        "        برمی‌گرداند:\n",
        "        لیستی از گرادیان‌ها برای هر وزن.\n",
        "        \"\"\"\n",
        "        gradients = [delta * input_ for input_ in self.inputs]  # محاسبه گرادیان‌ها\n",
        "        return gradients\n",
        "\n",
        "    def update_weights(self, learning_rate: float, gradients: List[float]):\n",
        "        \"\"\"\n",
        "        وزن‌ها و بایاس نورون را با استفاده از گرادیان‌ها و نرخ یادگیری به‌روزرسانی می‌کند.\n",
        "\n",
        "        آرگومان‌ها:\n",
        "        learning_rate: نرخ یادگیری برای به‌روزرسانی وزن‌ها.\n",
        "        gradients: گرادیان‌های محاسبه‌شده برای هر وزن.\n",
        "        \"\"\"\n",
        "        # به‌روزرسانی وزن‌ها\n",
        "        self.weights = [w - learning_rate * g for w, g in zip(self.weights, gradients)]\n",
        "        self.bias -= learning_rate * gradients[-1]  # بایاس را نیز به‌روزرسانی می‌کنیم\n",
        "\n",
        "    def propagate_error_back(self) -> List[float]:\n",
        "        \"\"\"\n",
        "        این متد خطای نورون رو برای لایه قبلی حساب می‌کنه.\n",
        "        یعنی اول مشتق سیگموید رو از خروجی خودش می‌گیره،\n",
        "        بعد با هر وزن ضرب می‌کنه تا بگه هر ورودی چقدر در خطا سهم داره.\n",
        "        \"\"\"\n",
        "        # مشتق سیگموید: output * (1 - output)\n",
        "        deriv = self.output * (1 - self.output)\n",
        "        # برای هر وزن، سهم خطا = مشتق * وزن\n",
        "        return [deriv * w for w in self.weights]"
      ],
      "metadata": {
        "id": "uJCkaPTT90u7"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# تست عملکرد کلاس نورون\n",
        "\n",
        "# ایجاد یک نورون با وزن‌ها و بایاس تصادفی\n",
        "weights = [random.uniform(-1, 1) for _ in range(3)]  # سه ورودی با وزن‌های تصادفی\n",
        "bias = random.uniform(-0.1, 0.1)  # بایاس تصادفی\n",
        "neuron = Neuron(weights, bias)\n",
        "\n",
        "# ورودی‌ها برای نورون\n",
        "inputs = [0.5, 0.2, 0.8]  # سه ورودی برای نورون\n",
        "\n",
        "# محاسبه خروجی نورون\n",
        "output = neuron.forward(inputs)\n",
        "print(f\"خروجی نورون قبل از فعال‌سازی: {output}\")\n",
        "\n",
        "# اعمال تابع فعال‌سازی (سیگموید)\n",
        "activated_output = neuron.activation(output)\n",
        "print(f\"خروجی نورون بعد از فعال‌سازی: {activated_output}\")\n",
        "\n",
        "# فرض می‌کنیم سیگنال خطا (دلتا) برابر 0.1 است\n",
        "delta = 0.1\n",
        "gradients = neuron.compute_gradient(delta)\n",
        "print(f\"گرادیان‌های محاسبه‌شده برای وزن‌ها: {gradients}\")\n",
        "\n",
        "# به‌روزرسانی وزن‌ها با نرخ یادگیری 0.01\n",
        "neuron.update_weights(learning_rate=0.01, gradients=gradients)\n",
        "print(f\"وزن‌ها و بایاس بعد از به‌روزرسانی: {neuron.weights}, {neuron.bias}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijZ-pMCIAScK",
        "outputId": "ec9777ae-406a-4ca0-b9b1-44a44d1ffe70"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "خروجی نورون قبل از فعال‌سازی: -1.077792653702492\n",
            "خروجی نورون بعد از فعال‌سازی: 0\n",
            "گرادیان‌های محاسبه‌شده برای وزن‌ها: [0.05, 0.020000000000000004, 0.08000000000000002]\n",
            "وزن‌ها و بایاس بعد از به‌روزرسانی: [-0.9662690086364676, 0.6989679522511152, -0.9533296253659759], 0.026481960458299727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "    \"\"\"یه لایه توی شبکه عصبی که ورودی‌ها رو می‌گیره، می‌فرسته توی نورون‌ها، فعال‌سازی می‌کنه و خروجی می‌سازه.\"\"\"\n",
        "\n",
        "    def __init__(self, neurons: List[Neuron], is_output_layer: bool = False):\n",
        "        \"\"\"\n",
        "        آرگومان‌ها:\n",
        "          neurons: لیستی از نورون‌های این لایه.\n",
        "          is_output_layer: اگه True باشه، این لایه آخره و خروجی‌شو با softmax می‌سازه.\n",
        "        \"\"\"\n",
        "        self.neurons = neurons\n",
        "        self.is_output_layer = is_output_layer\n",
        "\n",
        "    def forward(self, inputs: List[float]) -> List[float]:\n",
        "        \"\"\"\n",
        "        داده‌ها رو می‌فرسته توی هر نورون و خروجی خام هر نورون (logits) رو می‌گیره.\n",
        "        بعد اگه لایه آخری باشه softmax می‌زنه؛ وگرنه با متد activation خود نورون سیگموید می‌زنه.\n",
        "\n",
        "        inputs: لیست ورودی به این لایه (مثلاً خروجی لایه قبلی).\n",
        "        برمی‌گردونه: لیست خروجی نهایی این لایه.\n",
        "        \"\"\"\n",
        "        # logits یعنی «خروجی خام نورون قبل از فعال‌سازی»\n",
        "        logits = [neuron.forward(inputs) for neuron in self.neurons]\n",
        "\n",
        "        if self.is_output_layer:\n",
        "            # پیاده‌سازی دستی softmax: exp هر عدد / مجموع expها\n",
        "            # exp_vals = [math.exp(x) for x in logits]\n",
        "            # sum_exp = sum(exp_vals)\n",
        "            # return [v / sum_exp for v in exp_vals]\n",
        "            return self.softmax(logits)\n",
        "        else:\n",
        "            # برای لایه‌های میانی: هر نورون خودش متد activation داره\n",
        "            return [neuron.activation(x) for neuron, x in zip(self.neurons, logits)]\n",
        "\n",
        "    def softmax(self, outputs: List[float]) -> List[float]:\n",
        "        exps = [math.exp(x) for x in outputs]\n",
        "        sum_exps = sum(exps)\n",
        "        return [exp / sum_exps for exp in exps]\n",
        "\n",
        "    def backward(self, delta: List[float], learning_rate: float) -> List[int]:\n",
        "        \"\"\"\n",
        "        دلتا (خطا) از لایه بعدی رو می‌گیره و برای هر نورون:\n",
        "        1. با compute_gradient گرادیان‌ها رو محاسبه می‌کنه\n",
        "        2. با update_weights وزن و بایاس رو آپدیت می‌کنه\n",
        "        3. با propagate_error_back دلتای مربوط به ورودی‌ها رو دریافت می‌کنه\n",
        "        در نهایت همه دلتاها رو جمع می‌کنه و برمی‌گردونه برای لایه قبلی.\n",
        "\n",
        "        delta: لیست خطا برای هر نورون این لایه\n",
        "        learning_rate: نرخ یادگیری\n",
        "        برمی‌گردونه: لیست delta برای لایه قبلی\n",
        "        \"\"\"\n",
        "        propagated = []  # این لیست، هر عنصرش لیست خطاهایی‌یه که هر نورون برای ورودی‌ها تولید می‌کنه\n",
        "\n",
        "        for i, neuron in enumerate(self.neurons):\n",
        "            # 1. گرادیان رو خود نورون محاسبه می‌کنه\n",
        "            grads = neuron.compute_gradient(delta[i])\n",
        "\n",
        "            # 2. خود نورون وزن‌ها و بایاس رو آپدیت می‌کنه\n",
        "            neuron.update_weights(learning_rate, grads)\n",
        "\n",
        "            # 3. خطا برای ورودی‌های نورون (برای لایه قبلی)\n",
        "            propagated.append(neuron.propagate_error_back())\n",
        "\n",
        "        # جمع کردن دلتاها برای هر ورودی\n",
        "        # zip(*propagated) ردیف‌ها رو ستونی می‌کنه تا بتونیم جمع کنیم\n",
        "        prev_delta = [sum(x) for x in zip(*propagated)]\n",
        "        return prev_delta"
      ],
      "metadata": {
        "id": "JBJr0gtOE391"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neurons = [\n",
        "    Neuron(weights=[0.5, -0.4], bias=0.1),\n",
        "    Neuron(weights=[-1.0, 2.0], bias=0.2)\n",
        "]\n",
        "layer = Layer(neurons, is_output_layer=False)\n",
        "\n",
        "layer_output = layer.forward(inputs)\n",
        "print(\"خروجی لایه:\", layer_output)\n",
        "\n",
        "delta = [0.1, -0.2]\n",
        "prev_delta = layer.backward(delta, learning_rate=0.01)\n",
        "print(\"دلتا برای لایه قبلی:\", prev_delta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1y_V3HZ-PPVD",
        "outputId": "a7c434f9-64c1-4da8-aaa8-e796da234612"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "خروجی لایه: [0.27, 0.10000000000000003]\n",
            "دلتا برای لایه قبلی: [0.008541449999999978, 0.10115658000000005]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    \"\"\"\n",
        "    یک شبکه‌ی عصبی ساده که از چند لایه تشکیل شده.\n",
        "    ورودی رو می‌گیره، از توی نورون‌ها عبور می‌ده، و خروجی نهایی رو تولید می‌کنه.\n",
        "    بعدش با روش backpropagation وزن‌ها رو آپدیت می‌کنه.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layers: List[Layer], epochs: int, learning_rate: float):\n",
        "        self.layers = layers\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def forward(self, inputs: List[float]) -> List[float]:\n",
        "        \"\"\"\n",
        "        ورودی‌ها رو از طریق همه‌ی لایه‌ها عبور می‌ده و خروجی نهایی رو حساب می‌کنه.\n",
        "        \"\"\"\n",
        "        outputs = inputs\n",
        "        for layer in self.layers:\n",
        "            outputs = layer.forward(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def backward(self, targets: List[float], outputs: List[float]):\n",
        "        \"\"\"\n",
        "        این تابع خطا رو از خروجی نهایی محاسبه می‌کنه و به عقب برمی‌گردونه تا وزن‌ها آپدیت بشن.\n",
        "        \"\"\"\n",
        "        delta = self.loss_derivative(outputs, targets)\n",
        "        for layer in reversed(self.layers):\n",
        "            delta = layer.backward(delta, self.learning_rate)\n",
        "\n",
        "    def compute_loss(self, predicted: List[float], actual: List[float]) -> float:\n",
        "        \"\"\"\n",
        "        مقدار خطا رو بین خروجی پیش‌بینی‌شده و مقدار واقعی حساب می‌کنه.\n",
        "        \"\"\"\n",
        "        return LossFunction.cross_entropy(predicted, actual)\n",
        "\n",
        "    def loss_derivative(self, outputs: List[float], targets: List[float]) -> List[float]:\n",
        "        \"\"\"\n",
        "        مشتق تابع خطا رو حساب می‌کنه، یعنی اختلاف بین خروجی مدل و مقدار درست.\n",
        "        \"\"\"\n",
        "        return [pred - target for pred, target in zip(outputs, targets)]\n",
        "\n",
        "    def train(self, training_data: List[tuple]):\n",
        "        \"\"\"\n",
        "        شبکه رو با داده‌های آموزشی آموزش می‌ده. هر بار وزن‌ها رو آپدیت می‌کنه.\n",
        "        \"\"\"\n",
        "        num_samples = len(training_data)\n",
        "        for epoch in range(self.epochs):\n",
        "            total_loss = 0\n",
        "            random.shuffle(training_data)\n",
        "\n",
        "            for inputs, targets in training_data:\n",
        "                outputs = self.forward(inputs)\n",
        "                loss = self.compute_loss(outputs, targets)\n",
        "                total_loss += loss\n",
        "                self.backward(targets, outputs)\n",
        "\n",
        "            avg_loss = total_loss / num_samples\n",
        "            print(f\"Epoch {epoch + 1}/{self.epochs} complete. Average loss: {avg_loss:.4f}\")\n",
        "\n",
        "    def evaluate(self, test_data: List[tuple]) -> float:\n",
        "        \"\"\"\n",
        "        دقت مدل رو با داده‌های تست حساب می‌کنه.\n",
        "        \"\"\"\n",
        "        inputs_batch, targets_batch = zip(*test_data)\n",
        "        predictions = [self.forward(inputs) for inputs in inputs_batch]\n",
        "        accuracy = self.calculate_accuracy(predictions, targets_batch)\n",
        "        return accuracy\n",
        "\n",
        "    def predict(self, new_data: List[float]) -> List[float]:\n",
        "        \"\"\"\n",
        "        برای یک ورودی جدید، خروجی مدل رو پیش‌بینی می‌کنه.\n",
        "        \"\"\"\n",
        "        return self.forward(new_data)\n",
        "\n",
        "    def calculate_accuracy(self, predictions: List[List[float]], targets: List[List[float]]) -> float:\n",
        "        \"\"\"\n",
        "        دقت پیش‌بینی مدل رو نسبت به مقدار درست حساب می‌کنه.\n",
        "        \"\"\"\n",
        "        correct_predictions = 0\n",
        "        for pred, target in zip(predictions, targets):\n",
        "            predicted_class = np.argmax(pred)\n",
        "            true_class = np.argmax(target)\n",
        "            if predicted_class == true_class:\n",
        "                correct_predictions += 1\n",
        "        return correct_predictions / len(targets)\n",
        "\n",
        "    def save_weights(self, filename: str):\n",
        "        \"\"\"\n",
        "        وزن‌ها و بایاس‌های نورون‌ها رو توی یه فایل ذخیره می‌کنه تا بعدا بشه دوباره بارگذاریشون کرد.\n",
        "        \"\"\"\n",
        "        weights = [[neuron.weights for neuron in layer.neurons] for layer in self.layers]\n",
        "        biases = [[neuron.bias for neuron in layer.neurons] for layer in self.layers]\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump((weights, biases), f)\n",
        "\n",
        "    def load_weights(self, filename: str):\n",
        "        \"\"\"\n",
        "        وزن‌ها و بایاس‌ها رو از فایل می‌خونه و به شبکه اختصاص می‌ده.\n",
        "        \"\"\"\n",
        "        with open(filename, 'rb') as f:\n",
        "            weights, biases = pickle.load(f)\n",
        "        for layer, layer_weights, layer_biases in zip(self.layers, weights, biases):\n",
        "            for neuron, w, b in zip(layer.neurons, layer_weights, layer_biases):\n",
        "                neuron.weights = w\n",
        "                neuron.bias = b\n"
      ],
      "metadata": {
        "id": "g3Jq30B-J1Lk"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LossFunction:\n",
        "    \"\"\"کلاسی برای محاسبه انواع تابع هزینه.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def cross_entropy(predicted_outputs: List[float], actual_outputs: List[float]) -> float:\n",
        "        \"\"\"\n",
        "        🔹 این تابع برای classification استفاده می‌شه، مخصوصاً وقتی خروجی‌ها one-hot هستن.\n",
        "        🔹 از لگاریتم استفاده می‌کنه تا تفاوت بین احتمال پیش‌بینی‌شده و مقدار واقعی رو بسنجه.\n",
        "        \"\"\"\n",
        "        predicted_outputs = np.clip(predicted_outputs, 1e-12, 1 - 1e-12)\n",
        "        loss = -sum([actual * np.log(pred) for pred, actual in zip(predicted_outputs, actual_outputs)])\n",
        "        return loss / len(predicted_outputs)\n",
        "\n",
        "    @staticmethod\n",
        "    def mean_squared_error(predicted_outputs: List[float], actual_outputs: List[float]) -> float:\n",
        "        \"\"\"\n",
        "        🔸 این تابع برای regression استفاده می‌شه.\n",
        "        🔸 خطای مربعی بین پیش‌بینی و مقدار واقعی رو محاسبه می‌کنه.\n",
        "        \"\"\"\n",
        "        squared_errors = [(pred - actual) ** 2 for pred, actual in zip(predicted_outputs, actual_outputs)]\n",
        "        return sum(squared_errors) / len(squared_errors)\n"
      ],
      "metadata": {
        "id": "anKsHgS8N-Iv"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightsInitializer:\n",
        "    \"\"\"کلاسی برای مقداردهی اولیه به وزن‌ها.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def random_uniform(num_inputs: int) -> List[float]:\n",
        "        \"\"\"\n",
        "        🔹 مقداردهی اولیه به وزن‌ها به صورت تصادفی بین -0.1 تا 0.1.\n",
        "        🔹 برای اینکه شبکه از صفر شروع نکنه و بتونه مسیر یادگیری پیدا کنه.\n",
        "        \"\"\"\n",
        "        return [random.uniform(-0.5, 0.5) for _ in range(num_inputs)]\n"
      ],
      "metadata": {
        "id": "da7SbIFhuipG"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workflow 🔮: Architecture"
      ],
      "metadata": {
        "id": "Iz1cl6tesGG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(labels: list) -> list:\n",
        "    # Create a consistent label-to-index mapping\n",
        "    unique_labels = sorted(set(labels))\n",
        "    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    print(\"Label to Index Mapping:\")\n",
        "    for label, index in label_to_index.items():\n",
        "        print(f\"  {label}: {index}\")\n",
        "\n",
        "    # One-hot encode each label\n",
        "    encoded = []\n",
        "    for label in labels:\n",
        "        vec = [0] * len(unique_labels)\n",
        "        vec[label_to_index[label]] = 1\n",
        "        encoded.append(vec)\n",
        "\n",
        "    # Optional: show a sample\n",
        "    print(\"\\nSample One-Hot Encoded Vectors:\")\n",
        "    for i in range(min(3, len(encoded))):\n",
        "        print(f\"  {labels[i]} => {encoded[i]}\")\n",
        "\n",
        "    return encoded\n"
      ],
      "metadata": {
        "id": "HBiYFcTcaJ-C"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create input layer"
      ],
      "metadata": {
        "id": "r5fEpiz3swJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Iris Classification/Iris.csv'\n",
        "test_data_loading(file_path)\n",
        "\n",
        "data, labels = load_dataset(file_path)\n",
        "labels = one_hot_encode(labels)\n",
        "data = min_max_normalizer(data)\n",
        "\n",
        "\n",
        "dataset = list(zip(data, labels))\n",
        "test_split_data()\n",
        "train, val, test = split_data(dataset, training_size=0.5, validation_size=0.0)\n",
        "training_data, validation_data, test_data = split_data(dataset)\n",
        "\n",
        "print(training_data)\n",
        "print(validation_data)\n",
        "print(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HQOh2Wko-KW",
        "outputId": "85a2c2c2-3b90-4615-8951-865ef956a69b"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded and validated successfully.\n",
            "Label to Index Mapping:\n",
            "  Iris-setosa: 0\n",
            "  Iris-versicolor: 1\n",
            "  Iris-virginica: 2\n",
            "\n",
            "Sample One-Hot Encoded Vectors:\n",
            "  Iris-setosa => [1, 0, 0]\n",
            "  Iris-setosa => [1, 0, 0]\n",
            "  Iris-setosa => [1, 0, 0]\n",
            "❌ تست شکست خورد: خطا در تقسیم دیتا: تعداد نهایی با اولیه نمی‌خونه.\n",
            "[([0.8456375838926175, 0.5277777777777778, 0.3333333333333332, 0.6440677966101694, 0.7083333333333334], [0, 0, 1]), ([0.4966442953020134, 0.5833333333333334, 0.3749999999999999, 0.559322033898305, 0.5], [0, 1, 0]), ([0.2080536912751678, 0.30555555555555564, 0.5833333333333333, 0.0847457627118644, 0.12500000000000003], [1, 0, 0]), ([0.8523489932885906, 0.4999999999999999, 0.41666666666666663, 0.6610169491525424, 0.7083333333333334], [0, 0, 1]), ([0.47651006711409394, 0.4999999999999999, 0.3333333333333332, 0.5084745762711864, 0.5], [0, 1, 0]), ([0.4161073825503356, 0.4722222222222222, 0.0833333333333334, 0.5084745762711864, 0.375], [0, 1, 0]), ([0.1342281879194631, 0.30555555555555564, 0.5833333333333333, 0.11864406779661016, 0.04166666666666667], [1, 0, 0]), ([0.1610738255033557, 0.13888888888888887, 0.5833333333333333, 0.15254237288135591, 0.04166666666666667], [1, 0, 0]), ([0.31543624161073824, 0.08333333333333327, 0.5, 0.06779661016949151, 0.04166666666666667], [1, 0, 0]), ([0.24161073825503357, 0.3333333333333333, 0.6249999999999999, 0.05084745762711865, 0.04166666666666667], [1, 0, 0]), ([0.8389261744966443, 0.8055555555555556, 0.5, 0.847457627118644, 0.7083333333333334], [0, 0, 1]), ([0.18120805369127516, 0.25000000000000006, 0.6249999999999999, 0.0847457627118644, 0.04166666666666667], [1, 0, 0]), ([0.7583892617449665, 0.38888888888888895, 0.20833333333333331, 0.6779661016949152, 0.7916666666666666], [0, 0, 1]), ([0.8859060402684564, 0.5833333333333334, 0.3333333333333332, 0.7796610169491525, 0.8750000000000001], [0, 0, 1]), ([0.9530201342281879, 0.41666666666666663, 0.2916666666666667, 0.6949152542372881, 0.75], [0, 0, 1]), ([0.5570469798657718, 0.4722222222222222, 0.2916666666666667, 0.6949152542372881, 0.625], [0, 1, 0]), ([0.348993288590604, 0.7222222222222222, 0.4583333333333333, 0.6610169491525424, 0.5833333333333334], [0, 1, 0]), ([0.6241610738255033, 0.19444444444444448, 0.1249999999999999, 0.38983050847457623, 0.375], [0, 1, 0]), ([0.8322147651006712, 0.6666666666666666, 0.5416666666666665, 0.7966101694915254, 0.8333333333333334], [0, 0, 1]), ([0.48322147651006714, 0.5555555555555555, 0.20833333333333331, 0.6610169491525424, 0.5833333333333334], [0, 1, 0]), ([0.697986577181208, 0.611111111111111, 0.41666666666666663, 0.8135593220338982, 0.8750000000000001], [0, 0, 1]), ([0.9261744966442953, 0.4722222222222222, 0.41666666666666663, 0.6440677966101694, 0.7083333333333334], [0, 0, 1]), ([0.35570469798657717, 0.3333333333333333, 0.1249999999999999, 0.5084745762711864, 0.5], [0, 1, 0]), ([0.785234899328859, 0.9444444444444444, 0.7499999999999998, 0.9661016949152542, 0.8750000000000001], [0, 0, 1]), ([0.22818791946308725, 0.1666666666666668, 0.4583333333333333, 0.0847457627118644, 0.0], [1, 0, 0]), ([0.7651006711409396, 0.41666666666666663, 0.3333333333333332, 0.6949152542372881, 0.9583333333333333], [0, 0, 1]), ([0.6510067114093959, 0.5277777777777778, 0.3749999999999999, 0.559322033898305, 0.5], [0, 1, 0]), ([0.5704697986577181, 0.4722222222222222, 0.5833333333333333, 0.5932203389830508, 0.625], [0, 1, 0]), ([0.436241610738255, 0.6666666666666666, 0.4583333333333333, 0.576271186440678, 0.5416666666666666], [0, 1, 0]), ([0.14093959731543623, 0.22222222222222213, 0.7083333333333333, 0.0847457627118644, 0.12500000000000003], [1, 0, 0]), ([0.040268456375838924, 0.08333333333333327, 0.5833333333333333, 0.06779661016949151, 0.08333333333333333], [1, 0, 0]), ([0.8053691275167785, 0.7222222222222222, 0.5, 0.7966101694915254, 0.9166666666666666], [0, 0, 1]), ([0.11409395973154363, 0.22222222222222213, 0.6249999999999999, 0.06779661016949151, 0.08333333333333333], [1, 0, 0]), ([0.06040268456375839, 0.1666666666666668, 0.4583333333333333, 0.0847457627118644, 0.0], [1, 0, 0]), ([0.16778523489932887, 0.19444444444444448, 0.41666666666666663, 0.1016949152542373, 0.04166666666666667], [1, 0, 0]), ([0.5771812080536913, 0.6666666666666666, 0.4583333333333333, 0.6271186440677966, 0.5833333333333334], [0, 1, 0]), ([0.2751677852348993, 0.055555555555555594, 0.1249999999999999, 0.05084745762711865, 0.08333333333333333], [1, 0, 0]), ([0.5302013422818792, 0.38888888888888895, 0.25, 0.423728813559322, 0.375], [0, 1, 0]), ([0.912751677852349, 0.5555555555555555, 0.5833333333333333, 0.7796610169491525, 0.9583333333333333], [0, 0, 1]), ([0.6040268456375839, 0.3333333333333333, 0.25, 0.576271186440678, 0.4583333333333333], [0, 1, 0]), ([0.5838926174496645, 0.5555555555555555, 0.1249999999999999, 0.576271186440678, 0.5], [0, 1, 0]), ([0.4697986577181208, 0.44444444444444453, 0.5, 0.6440677966101694, 0.7083333333333334], [0, 1, 0]), ([0.4429530201342282, 0.361111111111111, 0.41666666666666663, 0.5932203389830508, 0.5833333333333334], [0, 1, 0]), ([0.9060402684563759, 0.9444444444444444, 0.41666666666666663, 0.8644067796610169, 0.9166666666666666], [0, 0, 1]), ([0.026845637583892617, 0.19444444444444448, 0.6666666666666666, 0.06779661016949151, 0.04166666666666667], [1, 0, 0]), ([0.1476510067114094, 0.08333333333333327, 0.6666666666666666, 0.0, 0.04166666666666667], [1, 0, 0]), ([0.7449664429530202, 0.5833333333333334, 0.2916666666666667, 0.7288135593220338, 0.75], [0, 0, 1]), ([0.6174496644295302, 0.41666666666666663, 0.25, 0.5084745762711864, 0.4583333333333333], [0, 1, 0]), ([0.8657718120805369, 0.8055555555555556, 0.41666666666666663, 0.8135593220338982, 0.625], [0, 0, 1]), ([0.46308724832214765, 0.361111111111111, 0.20833333333333331, 0.4915254237288135, 0.4166666666666667], [0, 1, 0]), ([0.9664429530201343, 0.6666666666666666, 0.5416666666666665, 0.7966101694915254, 1.0], [0, 0, 1]), ([0.959731543624161, 0.6944444444444443, 0.5, 0.8305084745762712, 0.9166666666666666], [0, 0, 1]), ([0.3624161073825503, 0.611111111111111, 0.3333333333333332, 0.6101694915254237, 0.5833333333333334], [0, 1, 0]), ([0.7114093959731543, 0.1666666666666668, 0.20833333333333331, 0.5932203389830508, 0.6666666666666666], [0, 0, 1]), ([0.8926174496644296, 0.5555555555555555, 0.3333333333333332, 0.6949152542372881, 0.5833333333333334], [0, 0, 1]), ([0.6845637583892618, 0.7777777777777776, 0.41666666666666663, 0.8305084745762712, 0.8333333333333334], [0, 0, 1]), ([0.3422818791946309, 0.5833333333333334, 0.5, 0.5932203389830508, 0.5833333333333334], [0, 1, 0]), ([0.6778523489932886, 0.41666666666666663, 0.2916666666666667, 0.6949152542372881, 0.75], [0, 0, 1]), ([0.42953020134228187, 0.361111111111111, 0.3749999999999999, 0.4406779661016949, 0.5], [0, 1, 0]), ([0.40939597315436244, 0.44444444444444453, 0.41666666666666663, 0.5423728813559322, 0.5833333333333334], [0, 1, 0]), ([0.40268456375838924, 0.19444444444444448, 0.0, 0.423728813559322, 0.375], [0, 1, 0]), ([0.5503355704697986, 0.41666666666666663, 0.2916666666666667, 0.4915254237288135, 0.4583333333333333], [0, 1, 0]), ([0.3691275167785235, 0.38888888888888895, 0.3333333333333332, 0.5932203389830508, 0.5], [0, 1, 0]), ([0.9798657718120806, 0.5555555555555555, 0.20833333333333331, 0.6779661016949152, 0.75], [0, 0, 1]), ([0.12080536912751678, 0.38888888888888895, 0.7499999999999998, 0.11864406779661016, 0.08333333333333333], [1, 0, 0]), ([0.9731543624161074, 0.6666666666666666, 0.41666666666666663, 0.711864406779661, 0.9166666666666666], [0, 0, 1]), ([0.37583892617449666, 0.5555555555555555, 0.5416666666666665, 0.6271186440677966, 0.625], [0, 1, 0]), ([0.44966442953020136, 0.41666666666666663, 0.2916666666666667, 0.5254237288135593, 0.375], [0, 1, 0]), ([0.3959731543624161, 0.25000000000000006, 0.2916666666666667, 0.4915254237288135, 0.5416666666666666], [0, 1, 0]), ([0.9395973154362416, 0.6666666666666666, 0.4583333333333333, 0.7796610169491525, 0.9583333333333333], [0, 0, 1]), ([0.87248322147651, 0.8611111111111112, 0.3333333333333332, 0.8644067796610169, 0.75], [0, 0, 1]), ([0.8590604026845637, 0.5833333333333334, 0.3333333333333332, 0.7796610169491525, 0.8333333333333334], [0, 0, 1]), ([0.5234899328859061, 0.4722222222222222, 0.3749999999999999, 0.5932203389830508, 0.5833333333333334], [0, 1, 0]), ([0.6644295302013423, 0.38888888888888895, 0.3333333333333332, 0.5254237288135593, 0.5], [0, 1, 0]), ([0.4563758389261745, 0.5277777777777778, 0.0833333333333334, 0.5932203389830508, 0.5833333333333334], [0, 1, 0]), ([0.4899328859060403, 0.4999999999999999, 0.3333333333333332, 0.6271186440677966, 0.4583333333333333], [0, 1, 0]), ([0.738255033557047, 0.611111111111111, 0.5, 0.6949152542372881, 0.7916666666666666], [0, 0, 1]), ([0.5637583892617449, 0.30555555555555564, 0.41666666666666663, 0.5932203389830508, 0.5833333333333334], [0, 1, 0]), ([0.6711409395973155, 0.5555555555555555, 0.5416666666666665, 0.847457627118644, 1.0], [0, 0, 1]), ([0.20134228187919462, 0.13888888888888887, 0.4583333333333333, 0.1016949152542373, 0.04166666666666667], [1, 0, 0]), ([0.0738255033557047, 0.13888888888888887, 0.5833333333333333, 0.1016949152542373, 0.04166666666666667], [1, 0, 0]), ([0.2214765100671141, 0.3333333333333333, 0.9166666666666666, 0.06779661016949151, 0.04166666666666667], [1, 0, 0]), ([0.09395973154362416, 0.41666666666666663, 0.8333333333333333, 0.033898305084745756, 0.04166666666666667], [1, 0, 0]), ([0.2483221476510067, 0.1666666666666668, 0.4583333333333333, 0.0847457627118644, 0.0], [1, 0, 0]), ([0.6912751677852349, 0.5555555555555555, 0.3749999999999999, 0.7796610169491525, 0.7083333333333334], [0, 0, 1]), ([0.04697986577181208, 0.19444444444444448, 0.5833333333333333, 0.0847457627118644, 0.04166666666666667], [1, 0, 0]), ([0.5973154362416108, 0.3333333333333333, 0.20833333333333331, 0.5084745762711864, 0.5], [0, 1, 0]), ([0.08053691275167785, 0.13888888888888887, 0.41666666666666663, 0.06779661016949151, 0.0], [1, 0, 0]), ([0.7718120805369127, 0.5833333333333334, 0.5, 0.7288135593220338, 0.9166666666666666], [0, 0, 1]), ([0.7919463087248322, 0.9444444444444444, 0.25, 1.0, 0.9166666666666666], [0, 0, 1]), ([0.8187919463087249, 0.9444444444444444, 0.3333333333333332, 0.9661016949152542, 0.7916666666666666], [0, 0, 1]), ([0.7181208053691275, 0.8333333333333333, 0.3749999999999999, 0.8983050847457626, 0.7083333333333334], [0, 0, 1]), ([0.10067114093959731, 0.38888888888888895, 1.0, 0.0847457627118644, 0.12500000000000003], [1, 0, 0]), ([0.26174496644295303, 0.22222222222222213, 0.5833333333333333, 0.0847457627118644, 0.04166666666666667], [1, 0, 0]), ([0.5369127516778524, 0.3333333333333333, 0.1666666666666666, 0.47457627118644063, 0.4166666666666667], [0, 1, 0]), ([0.4228187919463087, 0.4999999999999999, 0.3749999999999999, 0.6271186440677966, 0.5416666666666666], [0, 1, 0]), ([0.825503355704698, 0.5555555555555555, 0.2916666666666667, 0.6610169491525424, 0.7083333333333334], [0, 0, 1]), ([0.7248322147651006, 0.6666666666666666, 0.20833333333333331, 0.8135593220338982, 0.7083333333333334], [0, 0, 1]), ([0.19463087248322147, 0.11111111111111119, 0.5, 0.1016949152542373, 0.04166666666666667], [1, 0, 0]), ([0.9194630872483222, 0.5833333333333334, 0.4583333333333333, 0.7627118644067796, 0.7083333333333334], [0, 0, 1]), ([0.087248322147651, 0.0, 0.41666666666666663, 0.016949152542372895, 0.0], [1, 0, 0]), ([0.2348993288590604, 0.19444444444444448, 0.5, 0.033898305084745756, 0.04166666666666667], [1, 0, 0]), ([0.06711409395973154, 0.30555555555555564, 0.7083333333333333, 0.0847457627118644, 0.04166666666666667], [1, 0, 0]), ([0.38926174496644295, 0.6388888888888887, 0.3749999999999999, 0.6101694915254237, 0.5], [0, 1, 0]), ([0.8993288590604027, 0.4999999999999999, 0.25, 0.7796610169491525, 0.5416666666666666], [0, 0, 1])]\n",
            "[[]]\n",
            "[([0.3288590604026846, 0.19444444444444448, 0.5416666666666665, 0.06779661016949151, 0.04166666666666667], [1, 0, 0]), ([0.3087248322147651, 0.22222222222222213, 0.7499999999999998, 0.1016949152542373, 0.04166666666666667], [1, 0, 0]), ([0.053691275167785234, 0.027777777777777922, 0.3749999999999999, 0.06779661016949151, 0.04166666666666667], [1, 0, 0]), ([0.12751677852348994, 0.22222222222222213, 0.7499999999999998, 0.0847457627118644, 0.08333333333333333], [1, 0, 0]), ([0.2550335570469799, 0.027777777777777922, 0.41666666666666663, 0.05084745762711865, 0.04166666666666667], [1, 0, 0]), ([0.020134228187919462, 0.08333333333333327, 0.4583333333333333, 0.0847457627118644, 0.04166666666666667], [1, 0, 0]), ([0.013422818791946308, 0.11111111111111119, 0.5, 0.05084745762711865, 0.04166666666666667], [1, 0, 0]), ([0.8120805369127517, 0.361111111111111, 0.3333333333333332, 0.6610169491525424, 0.7916666666666666], [0, 0, 1]), ([0.33557046979865773, 0.7499999999999999, 0.5, 0.6271186440677966, 0.5416666666666666], [0, 1, 0]), ([0.2684563758389262, 0.19444444444444448, 0.6249999999999999, 0.05084745762711865, 0.08333333333333333], [1, 0, 0]), ([0.2953020134228188, 0.22222222222222213, 0.7499999999999998, 0.15254237288135591, 0.12500000000000003], [1, 0, 0]), ([0.30201342281879195, 0.13888888888888887, 0.41666666666666663, 0.06779661016949151, 0.08333333333333333], [1, 0, 0]), ([0.006711409395973154, 0.1666666666666668, 0.41666666666666663, 0.06779661016949151, 0.04166666666666667], [1, 0, 0]), ([0.3825503355704698, 0.1666666666666668, 0.1666666666666666, 0.38983050847457623, 0.375], [0, 1, 0]), ([0.21476510067114093, 0.25000000000000006, 0.8749999999999998, 0.0847457627118644, 0.0], [1, 0, 0]), ([0.3221476510067114, 0.27777777777777773, 0.7083333333333333, 0.0847457627118644, 0.04166666666666667], [1, 0, 0]), ([0.18791946308724833, 0.25000000000000006, 0.5833333333333333, 0.06779661016949151, 0.04166666666666667], [1, 0, 0]), ([0.7315436241610739, 0.8055555555555556, 0.6666666666666666, 0.8644067796610169, 1.0], [0, 0, 1]), ([0.5436241610738255, 0.3333333333333333, 0.1666666666666666, 0.4576271186440678, 0.375], [0, 1, 0]), ([0.6375838926174496, 0.38888888888888895, 0.41666666666666663, 0.5423728813559322, 0.4583333333333333], [0, 1, 0]), ([0.6577181208053692, 0.22222222222222213, 0.20833333333333331, 0.3389830508474576, 0.4166666666666667], [0, 1, 0]), ([1.0, 0.44444444444444453, 0.41666666666666663, 0.6949152542372881, 0.7083333333333334], [0, 0, 1]), ([0.03355704697986577, 0.30555555555555564, 0.7916666666666665, 0.11864406779661016, 0.12500000000000003], [1, 0, 0]), ([0.15436241610738255, 0.22222222222222213, 0.5416666666666665, 0.11864406779661016, 0.16666666666666669], [1, 0, 0]), ([0.28859060402684567, 0.19444444444444448, 0.6249999999999999, 0.1016949152542373, 0.20833333333333334], [1, 0, 0]), ([0.5167785234899329, 0.6666666666666666, 0.41666666666666663, 0.6779661016949152, 0.6666666666666666], [0, 1, 0]), ([0.174496644295302, 0.19444444444444448, 0.5833333333333333, 0.1016949152542373, 0.12500000000000003], [1, 0, 0]), ([0.9328859060402684, 0.7222222222222222, 0.4583333333333333, 0.7457627118644068, 0.8333333333333334], [0, 0, 1]), ([0.9932885906040269, 0.5277777777777778, 0.5833333333333333, 0.7457627118644068, 0.9166666666666666], [0, 0, 1]), ([0.9463087248322147, 0.7222222222222222, 0.4583333333333333, 0.6949152542372881, 0.9166666666666666], [0, 0, 1]), ([0.7046979865771812, 0.9166666666666665, 0.41666666666666663, 0.9491525423728813, 0.8333333333333334], [0, 0, 1]), ([0.0, 0.22222222222222213, 0.6249999999999999, 0.06779661016949151, 0.04166666666666667], [1, 0, 0]), ([0.5033557046979866, 0.6388888888888887, 0.41666666666666663, 0.576271186440678, 0.5416666666666666], [0, 1, 0]), ([0.5100671140939598, 0.6944444444444443, 0.3333333333333332, 0.6440677966101694, 0.5416666666666666], [0, 1, 0]), ([0.6442953020134228, 0.38888888888888895, 0.3749999999999999, 0.5423728813559322, 0.5], [0, 1, 0]), ([0.610738255033557, 0.4999999999999999, 0.41666666666666663, 0.6101694915254237, 0.5416666666666666], [0, 1, 0]), ([0.28187919463087246, 0.027777777777777922, 0.5, 0.05084745762711865, 0.04166666666666667], [1, 0, 0]), ([0.6308724832214765, 0.361111111111111, 0.2916666666666667, 0.5423728813559322, 0.5], [0, 1, 0]), ([0.5906040268456376, 0.361111111111111, 0.41666666666666663, 0.5254237288135593, 0.5], [0, 1, 0]), ([0.7785234899328859, 0.611111111111111, 0.41666666666666663, 0.7627118644067796, 0.7083333333333334], [0, 0, 1]), ([0.9865771812080537, 0.611111111111111, 0.41666666666666663, 0.711864406779661, 0.7916666666666666], [0, 0, 1]), ([0.8791946308724832, 1.0, 0.7499999999999998, 0.9152542372881356, 0.7916666666666666], [0, 0, 1]), ([0.7516778523489933, 0.6944444444444443, 0.41666666666666663, 0.7627118644067796, 0.8333333333333334], [0, 0, 1]), ([0.10738255033557047, 0.30555555555555564, 0.7916666666666665, 0.05084745762711865, 0.12500000000000003], [1, 0, 0]), ([0.7986577181208053, 0.4722222222222222, 0.0833333333333334, 0.6779661016949152, 0.5833333333333334], [0, 0, 1])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_layer = Layer(\n",
        "    neurons=[Neuron(WeightsInitializer.random_uniform(2)) for _ in range(4)]\n",
        ")\n",
        "\n",
        "hidden_layer_1 = Layer(\n",
        "    neurons=[Neuron(WeightsInitializer.random_uniform(4)) for _ in range(5)]\n",
        ")\n",
        "\n",
        "hidden_layer_2 = Layer(\n",
        "    neurons=[Neuron(WeightsInitializer.random_uniform(5)) for _ in range(4)]\n",
        ")\n",
        "\n",
        "output_layer = Layer(\n",
        "    neurons=[Neuron(WeightsInitializer.random_uniform(4)) for _ in range(3)],\n",
        "    is_output_layer=True\n",
        ")\n",
        "\n",
        "network = Network(\n",
        "    layers=[input_layer, hidden_layer_1, hidden_layer_2, output_layer],\n",
        "    epochs=1000,\n",
        "    learning_rate=0.05\n",
        ")\n",
        "\n",
        "network.train(train)\n",
        "accuracy = network.evaluate(test)\n",
        "print(f\"Test Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "# network.save_weights(\"iris_model.pkl\")\n",
        "# network.load_weights(\"iris_model.pkl\")"
      ],
      "metadata": {
        "id": "xrEKj4tMsYN8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}